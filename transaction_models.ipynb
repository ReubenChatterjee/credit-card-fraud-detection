{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses a wide variety of modeling algorithms for a binary classification problem. It reads a file created from a feature selection process that has a reasonably small number of good variables, and we know their order of multivariate importance because we used a proper wrapper method. We can explore # input variables, model algorithms and tune model hyperparameters. At the end we can select our favorite algorithm, run it again and build the final model performace score percentile tables.\n",
    "\n",
    "Here we call the larger fraction population the goods and the smaller fraction the bads. This notebook was originally\n",
    "written for fraud detection but can be used for any binary classification. It uses detection rate as an appropriate measure of goodness.\n",
    "\n",
    "Rather than use a built-in CV, we do a \"manual CV\" by running each model multiple (nitermax) times and average the performance on the training (trn), testing (tst) and out of time (oot) data sets.\n",
    "\n",
    "Some of the ML algorithms are very fast and some are slow. Feel free to comment out any cells/models you want. At the bottom of the notebook you can select your final model/hyperparameters to run one time only and then make the business perfoemance tables for that final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to add: better calculation of FDR@3% when building a model using sampled training data. Right now I just approximate it by using the entire population trntst for a model built with sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rchatterjee/.local/lib/python3.9/site-packages/dask/dataframe/__init__.py:31: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97496, 22)\n",
      "CPU times: user 185 ms, sys: 38.3 ms, total: 223 ms\n",
      "Wall time: 222 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>Card_dow_unique_count_for_merch_zip_7</th>\n",
       "      <th>Cardnum_actual/toal_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cardnum_unique_count_for_card_state_1  Card_Merchdesc_State_total_7  \\\n",
       "0                                      1                          3.62   \n",
       "1                                      1                         31.42   \n",
       "2                                      1                        178.49   \n",
       "3                                      1                          3.62   \n",
       "4                                      1                          7.24   \n",
       "\n",
       "   Cardnum_count_1_by_30  Cardnum_max_14  Card_dow_vdratio_0by60  \\\n",
       "0               0.033333            3.62                0.000011   \n",
       "1               0.033333           31.42                0.000011   \n",
       "2               0.033333          178.49                0.000011   \n",
       "3               0.033333            3.62                0.000011   \n",
       "4               0.033333            3.62                0.016667   \n",
       "\n",
       "   Card_dow_vdratio_0by14  Merchnum_desc_State_total_3  \\\n",
       "0                0.000049                         3.62   \n",
       "1                0.000049                        31.42   \n",
       "2                0.000049                       178.49   \n",
       "3                0.000049                         3.62   \n",
       "4                0.071429                         7.24   \n",
       "\n",
       "   Card_Merchdesc_total_7  Card_dow_unique_count_for_merch_zip_7  \\\n",
       "0                    3.62                                      1   \n",
       "1                   31.42                                      1   \n",
       "2                  178.49                                      1   \n",
       "3                    3.62                                      1   \n",
       "4                    7.24                                      1   \n",
       "\n",
       "   Cardnum_actual/toal_0  ...  Cardnum_unique_count_for_card_state_3  \\\n",
       "0                    1.0  ...                                      1   \n",
       "1                    1.0  ...                                      1   \n",
       "2                    1.0  ...                                      1   \n",
       "3                    1.0  ...                                      1   \n",
       "4                    0.5  ...                                      1   \n",
       "\n",
       "   Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "0                                    1                       3.62   \n",
       "1                                    1                      31.42   \n",
       "2                                    1                     178.49   \n",
       "3                                    1                       3.62   \n",
       "4                                    1                       7.24   \n",
       "\n",
       "   Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "0                                    1                    1.0   \n",
       "1                                    1                    1.0   \n",
       "2                                    1                    1.0   \n",
       "3                                    1                    1.0   \n",
       "4                                    1                    0.5   \n",
       "\n",
       "   Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "0                                      1                   1.0   \n",
       "1                                      1                   1.0   \n",
       "2                                      1                   1.0   \n",
       "3                                      1                   1.0   \n",
       "4                                      1                   1.0   \n",
       "\n",
       "   Card_dow_unique_count_for_merch_state_1  Recnum  Fraud  \n",
       "0                                        1       1      0  \n",
       "1                                        1       2      0  \n",
       "2                                        1       3      0  \n",
       "3                                        1       4      0  \n",
       "4                                        1       5      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vars = pd.read_csv('vars_final.csv')\n",
    "print(vars.shape)\n",
    "vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recnum',\n",
       " 'Fraud',\n",
       " 'Cardnum_unique_count_for_card_state_1',\n",
       " 'Card_Merchdesc_State_total_7',\n",
       " 'Cardnum_count_1_by_30',\n",
       " 'Cardnum_max_14',\n",
       " 'Card_dow_vdratio_0by60',\n",
       " 'Card_dow_vdratio_0by14',\n",
       " 'Merchnum_desc_State_total_3',\n",
       " 'Card_Merchdesc_total_7',\n",
       " 'Card_dow_unique_count_for_merch_zip_7',\n",
       " 'Cardnum_actual/toal_0',\n",
       " 'Card_dow_vdratio_0by7',\n",
       " 'Cardnum_vdratio_1by7',\n",
       " 'Cardnum_unique_count_for_card_state_3',\n",
       " 'Cardnum_unique_count_for_card_zip_3',\n",
       " 'Merchnum_desc_Zip_total_3',\n",
       " 'Cardnum_unique_count_for_Merchnum_3',\n",
       " 'Cardnum_actual/toal_1',\n",
       " 'Cardnum_unique_count_for_card_state_7',\n",
       " 'Cardnum_actual/max_0',\n",
       " 'Card_dow_unique_count_for_merch_state_1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the number of variables desired here, and set the names of the y and record number properly\n",
    "NVARS = 20\n",
    "\n",
    "# vars.rename(columns={'record':'Recnum'},inplace=True)\n",
    "# vars.rename(columns={'fraud_label':'Fraud'},inplace=True)\n",
    "detect_rate = .03\n",
    "numvars = min(NVARS,len(vars)-2)\n",
    "final_vars_list = ['Recnum','Fraud']\n",
    "for i in range(numvars):\n",
    "    final_vars_list.append(vars.columns[i])\n",
    "    \n",
    "final_vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recnum  Fraud  Cardnum_unique_count_for_card_state_1  \\\n",
       "0       1      0                                      1   \n",
       "1       2      0                                      1   \n",
       "2       3      0                                      1   \n",
       "3       4      0                                      1   \n",
       "4       5      0                                      1   \n",
       "\n",
       "   Card_Merchdesc_State_total_7  Cardnum_count_1_by_30  Cardnum_max_14  \\\n",
       "0                          3.62               0.033333            3.62   \n",
       "1                         31.42               0.033333           31.42   \n",
       "2                        178.49               0.033333          178.49   \n",
       "3                          3.62               0.033333            3.62   \n",
       "4                          7.24               0.033333            3.62   \n",
       "\n",
       "   Card_dow_vdratio_0by60  Card_dow_vdratio_0by14  \\\n",
       "0                0.000011                0.000049   \n",
       "1                0.000011                0.000049   \n",
       "2                0.000011                0.000049   \n",
       "3                0.000011                0.000049   \n",
       "4                0.016667                0.071429   \n",
       "\n",
       "   Merchnum_desc_State_total_3  Card_Merchdesc_total_7  ...  \\\n",
       "0                         3.62                    3.62  ...   \n",
       "1                        31.42                   31.42  ...   \n",
       "2                       178.49                  178.49  ...   \n",
       "3                         3.62                    3.62  ...   \n",
       "4                         7.24                    7.24  ...   \n",
       "\n",
       "   Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "0               0.000098              0.000098   \n",
       "1               0.000098              0.000098   \n",
       "2               0.000098              0.000098   \n",
       "3               0.000098              0.000098   \n",
       "4               0.142857              0.142857   \n",
       "\n",
       "   Cardnum_unique_count_for_card_state_3  Cardnum_unique_count_for_card_zip_3  \\\n",
       "0                                      1                                    1   \n",
       "1                                      1                                    1   \n",
       "2                                      1                                    1   \n",
       "3                                      1                                    1   \n",
       "4                                      1                                    1   \n",
       "\n",
       "   Merchnum_desc_Zip_total_3  Cardnum_unique_count_for_Merchnum_3  \\\n",
       "0                       3.62                                    1   \n",
       "1                      31.42                                    1   \n",
       "2                     178.49                                    1   \n",
       "3                       3.62                                    1   \n",
       "4                       7.24                                    1   \n",
       "\n",
       "   Cardnum_actual/toal_1  Cardnum_unique_count_for_card_state_7  \\\n",
       "0                    1.0                                      1   \n",
       "1                    1.0                                      1   \n",
       "2                    1.0                                      1   \n",
       "3                    1.0                                      1   \n",
       "4                    0.5                                      1   \n",
       "\n",
       "   Cardnum_actual/max_0  Card_dow_unique_count_for_merch_state_1  \n",
       "0                   1.0                                        1  \n",
       "1                   1.0                                        1  \n",
       "2                   1.0                                        1  \n",
       "3                   1.0                                        1  \n",
       "4                   1.0                                        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars = vars.filter(final_vars_list,axis=1)\n",
    "vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97496, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars['Fraud'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud rate in data is 0.020995733158283417\n"
     ]
    }
   ],
   "source": [
    "print(\"fraud rate in data is\",vars['Fraud'].sum()/len(vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>31.42</td>\n",
       "      <td>31.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31.42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>178.49</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>178.49</td>\n",
       "      <td>178.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>7.24</td>\n",
       "      <td>3.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230.32</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>230.32</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>230.32</td>\n",
       "      <td>230.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>230.32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.11</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>62.11</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>62.11</td>\n",
       "      <td>62.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.86</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>10.86</td>\n",
       "      <td>10.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recnum  Fraud  Cardnum_unique_count_for_card_state_1  \\\n",
       "0       1      0                                      1   \n",
       "1       2      0                                      1   \n",
       "2       3      0                                      1   \n",
       "3       4      0                                      1   \n",
       "4       5      0                                      1   \n",
       "5       6      0                                      1   \n",
       "6       7      0                                      1   \n",
       "7       8      0                                      1   \n",
       "8       9      0                                      1   \n",
       "9      10      0                                      1   \n",
       "\n",
       "   Card_Merchdesc_State_total_7  Cardnum_count_1_by_30  Cardnum_max_14  \\\n",
       "0                          3.62               0.033333            3.62   \n",
       "1                         31.42               0.033333           31.42   \n",
       "2                        178.49               0.033333          178.49   \n",
       "3                          3.62               0.033333            3.62   \n",
       "4                          7.24               0.033333            3.62   \n",
       "5                          3.67               0.033333            3.67   \n",
       "6                          3.62               0.033333            3.62   \n",
       "7                        230.32               0.033333          230.32   \n",
       "8                         62.11               0.033333           62.11   \n",
       "9                         10.86               0.033333            3.62   \n",
       "\n",
       "   Card_dow_vdratio_0by60  Card_dow_vdratio_0by14  \\\n",
       "0                0.000011                0.000049   \n",
       "1                0.000011                0.000049   \n",
       "2                0.000011                0.000049   \n",
       "3                0.000011                0.000049   \n",
       "4                0.016667                0.071429   \n",
       "5                0.000011                0.000049   \n",
       "6                0.000011                0.000049   \n",
       "7                0.000011                0.000049   \n",
       "8                0.000011                0.000049   \n",
       "9                0.016667                0.071429   \n",
       "\n",
       "   Merchnum_desc_State_total_3  Card_Merchdesc_total_7  ...  \\\n",
       "0                         3.62                    3.62  ...   \n",
       "1                        31.42                   31.42  ...   \n",
       "2                       178.49                  178.49  ...   \n",
       "3                         3.62                    3.62  ...   \n",
       "4                         7.24                    7.24  ...   \n",
       "5                         3.67                    3.67  ...   \n",
       "6                         7.24                    3.62  ...   \n",
       "7                       230.32                  230.32  ...   \n",
       "8                        62.11                   62.11  ...   \n",
       "9                        10.86                   10.86  ...   \n",
       "\n",
       "   Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "0               0.000098              0.000098   \n",
       "1               0.000098              0.000098   \n",
       "2               0.000098              0.000098   \n",
       "3               0.000098              0.000098   \n",
       "4               0.142857              0.142857   \n",
       "5               0.000098              0.000098   \n",
       "6               0.000098              0.000098   \n",
       "7               0.000098              0.000098   \n",
       "8               0.000098              0.000098   \n",
       "9               0.142857              0.142857   \n",
       "\n",
       "   Cardnum_unique_count_for_card_state_3  Cardnum_unique_count_for_card_zip_3  \\\n",
       "0                                      1                                    1   \n",
       "1                                      1                                    1   \n",
       "2                                      1                                    1   \n",
       "3                                      1                                    1   \n",
       "4                                      1                                    1   \n",
       "5                                      1                                    1   \n",
       "6                                      1                                    1   \n",
       "7                                      1                                    1   \n",
       "8                                      1                                    1   \n",
       "9                                      1                                    1   \n",
       "\n",
       "   Merchnum_desc_Zip_total_3  Cardnum_unique_count_for_Merchnum_3  \\\n",
       "0                       3.62                                    1   \n",
       "1                      31.42                                    1   \n",
       "2                     178.49                                    1   \n",
       "3                       3.62                                    1   \n",
       "4                       7.24                                    1   \n",
       "5                       3.67                                    1   \n",
       "6                       7.24                                    1   \n",
       "7                     230.32                                    1   \n",
       "8                      62.11                                    1   \n",
       "9                      10.86                                    1   \n",
       "\n",
       "   Cardnum_actual/toal_1  Cardnum_unique_count_for_card_state_7  \\\n",
       "0               1.000000                                      1   \n",
       "1               1.000000                                      1   \n",
       "2               1.000000                                      1   \n",
       "3               1.000000                                      1   \n",
       "4               0.500000                                      1   \n",
       "5               1.000000                                      1   \n",
       "6               1.000000                                      1   \n",
       "7               1.000000                                      1   \n",
       "8               1.000000                                      1   \n",
       "9               0.333333                                      1   \n",
       "\n",
       "   Cardnum_actual/max_0  Card_dow_unique_count_for_merch_state_1  \n",
       "0                   1.0                                        1  \n",
       "1                   1.0                                        1  \n",
       "2                   1.0                                        1  \n",
       "3                   1.0                                        1  \n",
       "4                   1.0                                        1  \n",
       "5                   1.0                                        1  \n",
       "6                   1.0                                        1  \n",
       "7                   1.0                                        1  \n",
       "8                   1.0                                        1  \n",
       "9                   1.0                                        1  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97496, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48915.137247</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>1.724266</td>\n",
       "      <td>676.571639</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>1194.385741</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>1413.472998</td>\n",
       "      <td>676.717953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047073</td>\n",
       "      <td>0.042583</td>\n",
       "      <td>2.169822</td>\n",
       "      <td>2.548279</td>\n",
       "      <td>1410.408067</td>\n",
       "      <td>2.633236</td>\n",
       "      <td>0.629566</td>\n",
       "      <td>3.010595</td>\n",
       "      <td>0.869332</td>\n",
       "      <td>1.443177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28262.212670</td>\n",
       "      <td>0.143371</td>\n",
       "      <td>1.568565</td>\n",
       "      <td>4074.921736</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>1856.894526</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.026914</td>\n",
       "      <td>5123.221915</td>\n",
       "      <td>4074.945381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058449</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>1.921436</td>\n",
       "      <td>2.842193</td>\n",
       "      <td>5120.097930</td>\n",
       "      <td>2.944757</td>\n",
       "      <td>0.396178</td>\n",
       "      <td>2.492444</td>\n",
       "      <td>0.289179</td>\n",
       "      <td>1.520407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24428.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>48916.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>801.660000</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>356.450000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>355.700000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.813617</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73402.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>1325.600000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>97852.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>47900.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>307302.580000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>307302.580000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Recnum         Fraud  Cardnum_unique_count_for_card_state_1  \\\n",
       "count  97496.000000  97496.000000                           97496.000000   \n",
       "mean   48915.137247      0.020996                               1.724266   \n",
       "std    28262.212670      0.143371                               1.568565   \n",
       "min        1.000000      0.000000                               1.000000   \n",
       "25%    24428.750000      0.000000                               1.000000   \n",
       "50%    48916.000000      0.000000                               1.000000   \n",
       "75%    73402.250000      0.000000                               2.000000   \n",
       "max    97852.000000      1.000000                              21.000000   \n",
       "\n",
       "       Card_Merchdesc_State_total_7  Cardnum_count_1_by_30  Cardnum_max_14  \\\n",
       "count                  97496.000000           97496.000000    97496.000000   \n",
       "mean                     676.571639               0.008823     1194.385741   \n",
       "std                     4074.921736               0.008372     1856.894526   \n",
       "min                        0.010000               0.000078        0.140000   \n",
       "25%                       52.500000               0.003226      307.000000   \n",
       "50%                      189.000000               0.005556      801.660000   \n",
       "75%                      590.000000               0.011111     1743.000000   \n",
       "max                   306633.410000               0.033333    47900.000000   \n",
       "\n",
       "       Card_dow_vdratio_0by60  Card_dow_vdratio_0by14  \\\n",
       "count            97496.000000            97496.000000   \n",
       "mean                 0.003231                0.020482   \n",
       "std                  0.004996                0.026914   \n",
       "min                  0.000007                0.000039   \n",
       "25%                  0.000152                0.001587   \n",
       "50%                  0.000370                0.002976   \n",
       "75%                  0.004762                0.042857   \n",
       "max                  0.016667                0.071429   \n",
       "\n",
       "       Merchnum_desc_State_total_3  Card_Merchdesc_total_7  ...  \\\n",
       "count                 97496.000000            97496.000000  ...   \n",
       "mean                   1413.472998              676.717953  ...   \n",
       "std                    5123.221915             4074.945381  ...   \n",
       "min                       0.010000                0.010000  ...   \n",
       "25%                      99.900000               52.500000  ...   \n",
       "50%                     356.450000              189.000000  ...   \n",
       "75%                    1325.600000              590.000000  ...   \n",
       "max                  307302.580000           306633.410000  ...   \n",
       "\n",
       "       Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "count           97496.000000          97496.000000   \n",
       "mean                0.047073              0.042583   \n",
       "std                 0.058449              0.042048   \n",
       "min                 0.000078              0.000079   \n",
       "25%                 0.003968              0.009524   \n",
       "50%                 0.008929              0.023810   \n",
       "75%                 0.107143              0.071429   \n",
       "max                 0.142857              0.142857   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_3  \\\n",
       "count                           97496.000000   \n",
       "mean                                2.169822   \n",
       "std                                 1.921436   \n",
       "min                                 1.000000   \n",
       "25%                                 1.000000   \n",
       "50%                                 2.000000   \n",
       "75%                                 3.000000   \n",
       "max                                22.000000   \n",
       "\n",
       "       Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "count                         97496.000000               97496.000000   \n",
       "mean                              2.548279                1410.408067   \n",
       "std                               2.842193                5120.097930   \n",
       "min                               1.000000                   0.010000   \n",
       "25%                               1.000000                  99.750000   \n",
       "50%                               2.000000                 355.700000   \n",
       "75%                               3.000000                1323.000000   \n",
       "max                              43.000000              307302.580000   \n",
       "\n",
       "       Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "count                         97496.000000           97496.000000   \n",
       "mean                              2.633236               0.629566   \n",
       "std                               2.944757               0.396178   \n",
       "min                               1.000000               0.000014   \n",
       "25%                               1.000000               0.201645   \n",
       "50%                               2.000000               0.813617   \n",
       "75%                               3.000000               1.000000   \n",
       "max                              45.000000               1.000000   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "count                           97496.000000          97496.000000   \n",
       "mean                                3.010595              0.869332   \n",
       "std                                 2.492444              0.289179   \n",
       "min                                 1.000000              0.000014   \n",
       "25%                                 1.000000              1.000000   \n",
       "50%                                 2.000000              1.000000   \n",
       "75%                                 4.000000              1.000000   \n",
       "max                                25.000000              1.000000   \n",
       "\n",
       "       Card_dow_unique_count_for_merch_state_1  \n",
       "count                             97496.000000  \n",
       "mean                                  1.443177  \n",
       "std                                   1.520407  \n",
       "min                                   1.000000  \n",
       "25%                                   1.000000  \n",
       "50%                                   1.000000  \n",
       "75%                                   1.000000  \n",
       "max                                  36.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recnum</th>\n",
       "      <th>Fraud</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84935</th>\n",
       "      <td>85265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174.61</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>225.00</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>174.61</td>\n",
       "      <td>174.61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>174.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84936</th>\n",
       "      <td>85266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>395.00</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>53.00</td>\n",
       "      <td>53.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>53.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84937</th>\n",
       "      <td>85267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>531.25</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>25.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84938</th>\n",
       "      <td>85268</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.17</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>395.04</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>97.17</td>\n",
       "      <td>97.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>97.17</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84939</th>\n",
       "      <td>85269</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.51</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>6.51</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>6.51</td>\n",
       "      <td>6.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84940</th>\n",
       "      <td>85270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>170.00</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>170.00</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>170.00</td>\n",
       "      <td>170.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84941</th>\n",
       "      <td>85271</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>250.00</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>250.00</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>250.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84942</th>\n",
       "      <td>85272</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>103.60</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84943</th>\n",
       "      <td>85273</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>2105.00</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>300.00</td>\n",
       "      <td>300.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>300.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84944</th>\n",
       "      <td>85274</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>133.20</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>268.20</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>133.20</td>\n",
       "      <td>133.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>133.20</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Recnum  Fraud  Cardnum_unique_count_for_card_state_1  \\\n",
       "84935   85265      0                                      1   \n",
       "84936   85266      0                                      1   \n",
       "84937   85267      0                                      1   \n",
       "84938   85268      0                                      1   \n",
       "84939   85269      0                                      1   \n",
       "84940   85270      0                                      1   \n",
       "84941   85271      0                                      1   \n",
       "84942   85272      0                                      1   \n",
       "84943   85273      0                                      1   \n",
       "84944   85274      0                                      1   \n",
       "\n",
       "       Card_Merchdesc_State_total_7  Cardnum_count_1_by_30  Cardnum_max_14  \\\n",
       "84935                        174.61               0.016667          225.00   \n",
       "84936                         53.00               0.002778          395.00   \n",
       "84937                         25.00               0.004762          531.25   \n",
       "84938                         97.17               0.004762          395.04   \n",
       "84939                          6.51               0.016667            6.51   \n",
       "84940                        170.00               0.016667          170.00   \n",
       "84941                        250.00               0.002564          250.00   \n",
       "84942                         15.00               0.016667          103.60   \n",
       "84943                        300.00               0.006667         2105.00   \n",
       "84944                        133.20               0.002564          268.20   \n",
       "\n",
       "       Card_dow_vdratio_0by60  Card_dow_vdratio_0by14  \\\n",
       "84935                0.000074                0.000317   \n",
       "84936                0.000278                0.001190   \n",
       "84937                0.000417                0.002976   \n",
       "84938                0.000194                0.001661   \n",
       "84939                0.000167                0.001429   \n",
       "84940                0.000260                0.001116   \n",
       "84941                0.000149                0.001786   \n",
       "84942                0.000167                0.001429   \n",
       "84943                0.000024                0.001984   \n",
       "84944                0.001042                0.004464   \n",
       "\n",
       "       Merchnum_desc_State_total_3  Card_Merchdesc_total_7  ...  \\\n",
       "84935                       174.61                  174.61  ...   \n",
       "84936                        53.00                   53.00  ...   \n",
       "84937                        25.00                   25.00  ...   \n",
       "84938                        97.17                   97.17  ...   \n",
       "84939                         6.51                    6.51  ...   \n",
       "84940                       170.00                  170.00  ...   \n",
       "84941                       250.00                  250.00  ...   \n",
       "84942                        15.00                   15.00  ...   \n",
       "84943                       300.00                  300.00  ...   \n",
       "84944                       133.20                  133.20  ...   \n",
       "\n",
       "       Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "84935               0.000635              0.010989   \n",
       "84936               0.009524              0.011905   \n",
       "84937               0.008929              0.008929   \n",
       "84938               0.003322              0.012987   \n",
       "84939               0.002857              0.005495   \n",
       "84940               0.002232              0.006803   \n",
       "84941               0.008929              0.011905   \n",
       "84942               0.002857              0.017857   \n",
       "84943               0.003968              0.009524   \n",
       "84944               0.008929              0.008929   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_3  \\\n",
       "84935                                      1   \n",
       "84936                                      1   \n",
       "84937                                      1   \n",
       "84938                                      1   \n",
       "84939                                      1   \n",
       "84940                                      1   \n",
       "84941                                      2   \n",
       "84942                                      2   \n",
       "84943                                      1   \n",
       "84944                                      2   \n",
       "\n",
       "       Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "84935                                    1                     174.61   \n",
       "84936                                    2                      53.00   \n",
       "84937                                    1                      25.00   \n",
       "84938                                    1                      97.17   \n",
       "84939                                    1                       6.51   \n",
       "84940                                    1                     170.00   \n",
       "84941                                    2                     250.00   \n",
       "84942                                    2                      15.00   \n",
       "84943                                    1                     300.00   \n",
       "84944                                    2                     133.20   \n",
       "\n",
       "       Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "84935                                    1                    1.0   \n",
       "84936                                    2                    1.0   \n",
       "84937                                    1                    1.0   \n",
       "84938                                    1                    1.0   \n",
       "84939                                    1                    1.0   \n",
       "84940                                    1                    1.0   \n",
       "84941                                    2                    1.0   \n",
       "84942                                    2                    1.0   \n",
       "84943                                    1                    1.0   \n",
       "84944                                    2                    1.0   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "84935                                      1                   1.0   \n",
       "84936                                      2                   1.0   \n",
       "84937                                      2                   1.0   \n",
       "84938                                      1                   1.0   \n",
       "84939                                      1                   1.0   \n",
       "84940                                      1                   1.0   \n",
       "84941                                      2                   1.0   \n",
       "84942                                      2                   1.0   \n",
       "84943                                      1                   1.0   \n",
       "84944                                      3                   1.0   \n",
       "\n",
       "       Card_dow_unique_count_for_merch_state_1  \n",
       "84935                                        1  \n",
       "84936                                        1  \n",
       "84937                                        1  \n",
       "84938                                        1  \n",
       "84939                                        1  \n",
       "84940                                        1  \n",
       "84941                                        1  \n",
       "84942                                        1  \n",
       "84943                                        1  \n",
       "84944                                        1  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the row i vars that corresponds to 11/1 for the oot\n",
    "test = vars[vars['Recnum'] > 85264]\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fraud\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_save = vars['Recnum']\n",
    "Y_save = pd.DataFrame(vars.loc[:,'Fraud'])\n",
    "Y_save.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and truncate field values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>Card_dow_unique_count_for_merch_zip_7</th>\n",
       "      <th>Cardnum_actual/toal_0</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>97496.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.724266</td>\n",
       "      <td>676.571639</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>1194.385741</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>1413.472998</td>\n",
       "      <td>676.717953</td>\n",
       "      <td>1.892016</td>\n",
       "      <td>0.765940</td>\n",
       "      <td>0.047073</td>\n",
       "      <td>0.042583</td>\n",
       "      <td>2.169822</td>\n",
       "      <td>2.548279</td>\n",
       "      <td>1410.408067</td>\n",
       "      <td>2.633236</td>\n",
       "      <td>0.629566</td>\n",
       "      <td>3.010595</td>\n",
       "      <td>0.869332</td>\n",
       "      <td>1.443177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.568565</td>\n",
       "      <td>4074.921736</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>1856.894526</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.026914</td>\n",
       "      <td>5123.221915</td>\n",
       "      <td>4074.945381</td>\n",
       "      <td>1.874101</td>\n",
       "      <td>0.355093</td>\n",
       "      <td>0.058449</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>1.921436</td>\n",
       "      <td>2.842193</td>\n",
       "      <td>5120.097930</td>\n",
       "      <td>2.944757</td>\n",
       "      <td>0.396178</td>\n",
       "      <td>2.492444</td>\n",
       "      <td>0.289179</td>\n",
       "      <td>1.520407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>801.660000</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>356.450000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>355.700000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.813617</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>1743.000000</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>1325.600000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>47900.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>307302.580000</td>\n",
       "      <td>306633.410000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>307302.580000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_unique_count_for_card_state_1  Card_Merchdesc_State_total_7  \\\n",
       "count                           97496.000000                  97496.000000   \n",
       "mean                                1.724266                    676.571639   \n",
       "std                                 1.568565                   4074.921736   \n",
       "min                                 1.000000                      0.010000   \n",
       "25%                                 1.000000                     52.500000   \n",
       "50%                                 1.000000                    189.000000   \n",
       "75%                                 2.000000                    590.000000   \n",
       "max                                21.000000                 306633.410000   \n",
       "\n",
       "       Cardnum_count_1_by_30  Cardnum_max_14  Card_dow_vdratio_0by60  \\\n",
       "count           97496.000000    97496.000000            97496.000000   \n",
       "mean                0.008823     1194.385741                0.003231   \n",
       "std                 0.008372     1856.894526                0.004996   \n",
       "min                 0.000078        0.140000                0.000007   \n",
       "25%                 0.003226      307.000000                0.000152   \n",
       "50%                 0.005556      801.660000                0.000370   \n",
       "75%                 0.011111     1743.000000                0.004762   \n",
       "max                 0.033333    47900.000000                0.016667   \n",
       "\n",
       "       Card_dow_vdratio_0by14  Merchnum_desc_State_total_3  \\\n",
       "count            97496.000000                 97496.000000   \n",
       "mean                 0.020482                  1413.472998   \n",
       "std                  0.026914                  5123.221915   \n",
       "min                  0.000039                     0.010000   \n",
       "25%                  0.001587                    99.900000   \n",
       "50%                  0.002976                   356.450000   \n",
       "75%                  0.042857                  1325.600000   \n",
       "max                  0.071429                307302.580000   \n",
       "\n",
       "       Card_Merchdesc_total_7  Card_dow_unique_count_for_merch_zip_7  \\\n",
       "count            97496.000000                           97496.000000   \n",
       "mean               676.717953                               1.892016   \n",
       "std               4074.945381                               1.874101   \n",
       "min                  0.010000                               1.000000   \n",
       "25%                 52.500000                               1.000000   \n",
       "50%                189.000000                               1.000000   \n",
       "75%                590.000000                               2.000000   \n",
       "max             306633.410000                              38.000000   \n",
       "\n",
       "       Cardnum_actual/toal_0  Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "count           97496.000000           97496.000000          97496.000000   \n",
       "mean                0.765940               0.047073              0.042583   \n",
       "std                 0.355093               0.058449              0.042048   \n",
       "min                 0.000014               0.000078              0.000079   \n",
       "25%                 0.500000               0.003968              0.009524   \n",
       "50%                 1.000000               0.008929              0.023810   \n",
       "75%                 1.000000               0.107143              0.071429   \n",
       "max                 1.000000               0.142857              0.142857   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_3  \\\n",
       "count                           97496.000000   \n",
       "mean                                2.169822   \n",
       "std                                 1.921436   \n",
       "min                                 1.000000   \n",
       "25%                                 1.000000   \n",
       "50%                                 2.000000   \n",
       "75%                                 3.000000   \n",
       "max                                22.000000   \n",
       "\n",
       "       Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "count                         97496.000000               97496.000000   \n",
       "mean                              2.548279                1410.408067   \n",
       "std                               2.842193                5120.097930   \n",
       "min                               1.000000                   0.010000   \n",
       "25%                               1.000000                  99.750000   \n",
       "50%                               2.000000                 355.700000   \n",
       "75%                               3.000000                1323.000000   \n",
       "max                              43.000000              307302.580000   \n",
       "\n",
       "       Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "count                         97496.000000           97496.000000   \n",
       "mean                              2.633236               0.629566   \n",
       "std                               2.944757               0.396178   \n",
       "min                               1.000000               0.000014   \n",
       "25%                               1.000000               0.201645   \n",
       "50%                               2.000000               0.813617   \n",
       "75%                               3.000000               1.000000   \n",
       "max                              45.000000               1.000000   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "count                           97496.000000          97496.000000   \n",
       "mean                                3.010595              0.869332   \n",
       "std                                 2.492444              0.289179   \n",
       "min                                 1.000000              0.000014   \n",
       "25%                                 1.000000              1.000000   \n",
       "50%                                 2.000000              1.000000   \n",
       "75%                                 4.000000              1.000000   \n",
       "max                                25.000000              1.000000   \n",
       "\n",
       "       Card_dow_unique_count_for_merch_state_1  \n",
       "count                             97496.000000  \n",
       "mean                                  1.443177  \n",
       "std                                   1.520407  \n",
       "min                                   1.000000  \n",
       "25%                                   1.000000  \n",
       "50%                                   1.000000  \n",
       "75%                                   1.000000  \n",
       "max                                  36.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_no_scaling = vars.drop(columns = ['Recnum','Fraud'])\n",
    "X_no_scaling.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = (X_no_scaling - X_no_scaling.mean()) / X_no_scaling.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to cap variables. For some problems it helps\n",
    "Clip = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>Card_dow_unique_count_for_merch_zip_7</th>\n",
       "      <th>Cardnum_actual/toal_0</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>97496.000000</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "      <td>9.749600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.469048e-17</td>\n",
       "      <td>2.623650e-18</td>\n",
       "      <td>3.148380e-17</td>\n",
       "      <td>1.836555e-17</td>\n",
       "      <td>-1.588766e-17</td>\n",
       "      <td>3.410745e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.538817e-18</td>\n",
       "      <td>1.107763e-17</td>\n",
       "      <td>2.448740e-17</td>\n",
       "      <td>6.996400e-18</td>\n",
       "      <td>8.745500e-19</td>\n",
       "      <td>3.906323e-17</td>\n",
       "      <td>4.081233e-17</td>\n",
       "      <td>2.332133e-18</td>\n",
       "      <td>9.911567e-18</td>\n",
       "      <td>-6.996400e-18</td>\n",
       "      <td>-5.830333e-18</td>\n",
       "      <td>5.014087e-17</td>\n",
       "      <td>2.623650e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.651796e-01</td>\n",
       "      <td>-3.346282e-01</td>\n",
       "      <td>-1.044444e+00</td>\n",
       "      <td>-7.194261e-01</td>\n",
       "      <td>-6.452996e-01</td>\n",
       "      <td>-7.595529e-01</td>\n",
       "      <td>-0.431853</td>\n",
       "      <td>-3.346955e-01</td>\n",
       "      <td>-5.031954e-01</td>\n",
       "      <td>-2.156969e+00</td>\n",
       "      <td>-8.040309e-01</td>\n",
       "      <td>-1.010863e+00</td>\n",
       "      <td>-6.089466e-01</td>\n",
       "      <td>-5.549694e-01</td>\n",
       "      <td>-4.316361e-01</td>\n",
       "      <td>-5.657259e-01</td>\n",
       "      <td>-1.589062e+00</td>\n",
       "      <td>-8.066762e-01</td>\n",
       "      <td>-3.006156e+00</td>\n",
       "      <td>-3.248492e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.651796e-01</td>\n",
       "      <td>-3.065413e-01</td>\n",
       "      <td>-6.685133e-01</td>\n",
       "      <td>-5.318162e-01</td>\n",
       "      <td>-6.163864e-01</td>\n",
       "      <td>-7.020303e-01</td>\n",
       "      <td>-0.399940</td>\n",
       "      <td>-3.066096e-01</td>\n",
       "      <td>-5.031954e-01</td>\n",
       "      <td>-7.489285e-01</td>\n",
       "      <td>-7.374772e-01</td>\n",
       "      <td>-7.862338e-01</td>\n",
       "      <td>-6.089466e-01</td>\n",
       "      <td>-5.549694e-01</td>\n",
       "      <td>-3.997146e-01</td>\n",
       "      <td>-5.657259e-01</td>\n",
       "      <td>-1.080121e+00</td>\n",
       "      <td>-8.066762e-01</td>\n",
       "      <td>4.518574e-01</td>\n",
       "      <td>-3.248492e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.651796e-01</td>\n",
       "      <td>-2.335016e-01</td>\n",
       "      <td>-3.902418e-01</td>\n",
       "      <td>-2.293881e-01</td>\n",
       "      <td>-5.725765e-01</td>\n",
       "      <td>-6.504263e-01</td>\n",
       "      <td>-0.317979</td>\n",
       "      <td>-2.335723e-01</td>\n",
       "      <td>-5.031954e-01</td>\n",
       "      <td>6.591519e-01</td>\n",
       "      <td>-6.526120e-01</td>\n",
       "      <td>-4.464873e-01</td>\n",
       "      <td>-8.838071e-02</td>\n",
       "      <td>-1.952418e-01</td>\n",
       "      <td>-3.177984e-01</td>\n",
       "      <td>-2.180500e-01</td>\n",
       "      <td>4.645672e-01</td>\n",
       "      <td>-4.054636e-01</td>\n",
       "      <td>4.518574e-01</td>\n",
       "      <td>-3.248492e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.782986e-01</td>\n",
       "      <td>-1.893059e-02</td>\n",
       "      <td>2.733285e-01</td>\n",
       "      <td>3.461339e-01</td>\n",
       "      <td>3.065090e-01</td>\n",
       "      <td>8.313456e-01</td>\n",
       "      <td>-0.008358</td>\n",
       "      <td>-1.900840e-02</td>\n",
       "      <td>6.696772e-02</td>\n",
       "      <td>6.591519e-01</td>\n",
       "      <td>1.027719e+00</td>\n",
       "      <td>6.860009e-01</td>\n",
       "      <td>4.321852e-01</td>\n",
       "      <td>1.644858e-01</td>\n",
       "      <td>-8.216512e-03</td>\n",
       "      <td>1.296260e-01</td>\n",
       "      <td>9.350196e-01</td>\n",
       "      <td>3.969616e-01</td>\n",
       "      <td>4.518574e-01</td>\n",
       "      <td>-3.248492e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.009425e+01</td>\n",
       "      <td>2.183188e+01</td>\n",
       "      <td>2.927610e+00</td>\n",
       "      <td>1.136351e+01</td>\n",
       "      <td>2.689572e+00</td>\n",
       "      <td>1.892914e+00</td>\n",
       "      <td>16.387194</td>\n",
       "      <td>2.183128e+01</td>\n",
       "      <td>1.069083e+01</td>\n",
       "      <td>6.591519e-01</td>\n",
       "      <td>1.638749e+00</td>\n",
       "      <td>2.384733e+00</td>\n",
       "      <td>1.000236e+01</td>\n",
       "      <td>1.022614e+01</td>\n",
       "      <td>1.640650e+01</td>\n",
       "      <td>1.024032e+01</td>\n",
       "      <td>9.350196e-01</td>\n",
       "      <td>8.822426e+00</td>\n",
       "      <td>4.518574e-01</td>\n",
       "      <td>1.153660e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_unique_count_for_card_state_1  Card_Merchdesc_State_total_7  \\\n",
       "count                           9.749600e+04                  9.749600e+04   \n",
       "mean                           -3.469048e-17                  2.623650e-18   \n",
       "std                             1.000000e+00                  1.000000e+00   \n",
       "min                            -4.651796e-01                 -3.346282e-01   \n",
       "25%                            -4.651796e-01                 -3.065413e-01   \n",
       "50%                            -4.651796e-01                 -2.335016e-01   \n",
       "75%                             1.782986e-01                 -1.893059e-02   \n",
       "max                             1.009425e+01                  2.183188e+01   \n",
       "\n",
       "       Cardnum_count_1_by_30  Cardnum_max_14  Card_dow_vdratio_0by60  \\\n",
       "count           9.749600e+04    9.749600e+04            9.749600e+04   \n",
       "mean            3.148380e-17    1.836555e-17           -1.588766e-17   \n",
       "std             1.000000e+00    1.000000e+00            1.000000e+00   \n",
       "min            -1.044444e+00   -7.194261e-01           -6.452996e-01   \n",
       "25%            -6.685133e-01   -5.318162e-01           -6.163864e-01   \n",
       "50%            -3.902418e-01   -2.293881e-01           -5.725765e-01   \n",
       "75%             2.733285e-01    3.461339e-01            3.065090e-01   \n",
       "max             2.927610e+00    1.136351e+01            2.689572e+00   \n",
       "\n",
       "       Card_dow_vdratio_0by14  Merchnum_desc_State_total_3  \\\n",
       "count            9.749600e+04                 97496.000000   \n",
       "mean             3.410745e-17                     0.000000   \n",
       "std              1.000000e+00                     1.000000   \n",
       "min             -7.595529e-01                    -0.431853   \n",
       "25%             -7.020303e-01                    -0.399940   \n",
       "50%             -6.504263e-01                    -0.317979   \n",
       "75%              8.313456e-01                    -0.008358   \n",
       "max              1.892914e+00                    16.387194   \n",
       "\n",
       "       Card_Merchdesc_total_7  Card_dow_unique_count_for_merch_zip_7  \\\n",
       "count            9.749600e+04                           9.749600e+04   \n",
       "mean            -5.538817e-18                           1.107763e-17   \n",
       "std              1.000000e+00                           1.000000e+00   \n",
       "min             -3.346955e-01                          -5.031954e-01   \n",
       "25%             -3.066096e-01                          -5.031954e-01   \n",
       "50%             -2.335723e-01                          -5.031954e-01   \n",
       "75%             -1.900840e-02                           6.696772e-02   \n",
       "max              2.183128e+01                           1.069083e+01   \n",
       "\n",
       "       Cardnum_actual/toal_0  Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "count           9.749600e+04           9.749600e+04          9.749600e+04   \n",
       "mean            2.448740e-17           6.996400e-18          8.745500e-19   \n",
       "std             1.000000e+00           1.000000e+00          1.000000e+00   \n",
       "min            -2.156969e+00          -8.040309e-01         -1.010863e+00   \n",
       "25%            -7.489285e-01          -7.374772e-01         -7.862338e-01   \n",
       "50%             6.591519e-01          -6.526120e-01         -4.464873e-01   \n",
       "75%             6.591519e-01           1.027719e+00          6.860009e-01   \n",
       "max             6.591519e-01           1.638749e+00          2.384733e+00   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_3  \\\n",
       "count                           9.749600e+04   \n",
       "mean                            3.906323e-17   \n",
       "std                             1.000000e+00   \n",
       "min                            -6.089466e-01   \n",
       "25%                            -6.089466e-01   \n",
       "50%                            -8.838071e-02   \n",
       "75%                             4.321852e-01   \n",
       "max                             1.000236e+01   \n",
       "\n",
       "       Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "count                         9.749600e+04               9.749600e+04   \n",
       "mean                          4.081233e-17               2.332133e-18   \n",
       "std                           1.000000e+00               1.000000e+00   \n",
       "min                          -5.549694e-01              -4.316361e-01   \n",
       "25%                          -5.549694e-01              -3.997146e-01   \n",
       "50%                          -1.952418e-01              -3.177984e-01   \n",
       "75%                           1.644858e-01              -8.216512e-03   \n",
       "max                           1.022614e+01               1.640650e+01   \n",
       "\n",
       "       Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "count                         9.749600e+04           9.749600e+04   \n",
       "mean                          9.911567e-18          -6.996400e-18   \n",
       "std                           1.000000e+00           1.000000e+00   \n",
       "min                          -5.657259e-01          -1.589062e+00   \n",
       "25%                          -5.657259e-01          -1.080121e+00   \n",
       "50%                          -2.180500e-01           4.645672e-01   \n",
       "75%                           1.296260e-01           9.350196e-01   \n",
       "max                           1.024032e+01           9.350196e-01   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "count                           9.749600e+04          9.749600e+04   \n",
       "mean                           -5.830333e-18          5.014087e-17   \n",
       "std                             1.000000e+00          1.000000e+00   \n",
       "min                            -8.066762e-01         -3.006156e+00   \n",
       "25%                            -8.066762e-01          4.518574e-01   \n",
       "50%                            -4.054636e-01          4.518574e-01   \n",
       "75%                             3.969616e-01          4.518574e-01   \n",
       "max                             8.822426e+00          4.518574e-01   \n",
       "\n",
       "       Card_dow_unique_count_for_merch_state_1  \n",
       "count                             9.749600e+04  \n",
       "mean                              2.623650e-18  \n",
       "std                               1.000000e+00  \n",
       "min                              -3.248492e-01  \n",
       "25%                              -3.248492e-01  \n",
       "50%                              -3.248492e-01  \n",
       "75%                              -3.248492e-01  \n",
       "max                               1.153660e+01  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push in any outlier values, then rescale\n",
    "X.clip(-1*Clip,Clip,inplace=True)\n",
    "X = (X - X.mean()) / X.std()\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraud    297\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate data into modeling (traintest) and out of time. Here I'm using the record number to do this separation.\n",
    "# you need to change this oot record number to whatever is appropriate for your data\n",
    "oot_recnum = 85264\n",
    "X_trntst = X[0:oot_recnum]\n",
    "Y_trntst = Y_save[0:oot_recnum]\n",
    "X_oot = X[oot_recnum:]\n",
    "Y_oot = Y_save[oot_recnum:]\n",
    "Y_oot.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_trntst.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trntst_save = X_trntst.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components = .999, svd_solver = 'full')\n",
    "pca.fit(X_trntst)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Number of components minus 1')\n",
    "plt.ylabel('PC variance')\n",
    "plt.xticks(np.arange(0, len(X_trntst.columns), step=2))\n",
    "plt.axhline(y=0,xmin=0,xmax=len(X_trntst.columns))\n",
    "X_trntst = X_trntst_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.379442</td>\n",
       "      <td>-0.599588</td>\n",
       "      <td>-0.281669</td>\n",
       "      <td>-2.548869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.375675</td>\n",
       "      <td>-0.574965</td>\n",
       "      <td>-0.271029</td>\n",
       "      <td>-2.543657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.355746</td>\n",
       "      <td>-0.444701</td>\n",
       "      <td>-0.214742</td>\n",
       "      <td>-2.516085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.379442</td>\n",
       "      <td>-0.599588</td>\n",
       "      <td>-0.281669</td>\n",
       "      <td>-2.548869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.189104</td>\n",
       "      <td>0.688353</td>\n",
       "      <td>-4.454551</td>\n",
       "      <td>-2.620904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4\n",
       "0 -2.379442 -0.599588 -0.281669 -2.548869\n",
       "1 -2.375675 -0.574965 -0.271029 -2.543657\n",
       "2 -2.355746 -0.444701 -0.214742 -2.516085\n",
       "3 -2.379442 -0.599588 -0.281669 -2.548869\n",
       "4  1.189104  0.688353 -4.454551 -2.620904"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We look at the above picture, select how many PCs we want to keep, and then redo the PCA with just this many PCs\n",
    "pca = PCA(n_components = 4, svd_solver = 'full')\n",
    "princ_comps = pca.fit_transform(X_trntst)\n",
    "X_trntst_pca = pd.DataFrame(princ_comps, columns = ['PC' + str(i) for i in range(1, pca.n_components_+1)])\n",
    "X_trntst_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85264</th>\n",
       "      <td>1.105279</td>\n",
       "      <td>-0.950573</td>\n",
       "      <td>0.118330</td>\n",
       "      <td>0.890316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85265</th>\n",
       "      <td>2.259955</td>\n",
       "      <td>0.488388</td>\n",
       "      <td>-4.457681</td>\n",
       "      <td>-1.582372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85266</th>\n",
       "      <td>-2.107495</td>\n",
       "      <td>1.730460</td>\n",
       "      <td>1.050826</td>\n",
       "      <td>-0.366579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85267</th>\n",
       "      <td>-0.732057</td>\n",
       "      <td>-0.461047</td>\n",
       "      <td>0.127960</td>\n",
       "      <td>0.608133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85268</th>\n",
       "      <td>-2.220142</td>\n",
       "      <td>-0.126191</td>\n",
       "      <td>0.455281</td>\n",
       "      <td>0.113203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PC1       PC2       PC3       PC4\n",
       "85264  1.105279 -0.950573  0.118330  0.890316\n",
       "85265  2.259955  0.488388 -4.457681 -1.582372\n",
       "85266 -2.107495  1.730460  1.050826 -0.366579\n",
       "85267 -0.732057 -0.461047  0.127960  0.608133\n",
       "85268 -2.220142 -0.126191  0.455281  0.113203"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "princ_comps = pca.transform(X_oot)\n",
    "X_oot_orig_pca = pd.DataFrame(princ_comps, columns = ['PC' + str(i) for i in range(1, pca.n_components_+1)],index=X_oot.index)\n",
    "X_oot_orig_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>Card_dow_unique_count_for_merch_zip_7</th>\n",
       "      <th>Cardnum_actual/toal_0</th>\n",
       "      <th>Card_dow_vdratio_0by7</th>\n",
       "      <th>Cardnum_vdratio_1by7</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "      <td>85264.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>-0.013411</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>-0.007076</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>0.005786</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>-0.009701</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.007299</td>\n",
       "      <td>1.029148</td>\n",
       "      <td>0.996083</td>\n",
       "      <td>0.988241</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>1.000623</td>\n",
       "      <td>1.038863</td>\n",
       "      <td>1.029151</td>\n",
       "      <td>1.014974</td>\n",
       "      <td>1.003383</td>\n",
       "      <td>1.000962</td>\n",
       "      <td>0.995805</td>\n",
       "      <td>1.007166</td>\n",
       "      <td>1.006367</td>\n",
       "      <td>1.038809</td>\n",
       "      <td>1.006149</td>\n",
       "      <td>1.001578</td>\n",
       "      <td>1.009276</td>\n",
       "      <td>1.000119</td>\n",
       "      <td>1.013853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.465180</td>\n",
       "      <td>-0.334628</td>\n",
       "      <td>-1.044444</td>\n",
       "      <td>-0.719426</td>\n",
       "      <td>-0.645300</td>\n",
       "      <td>-0.759504</td>\n",
       "      <td>-0.431853</td>\n",
       "      <td>-0.334696</td>\n",
       "      <td>-0.503195</td>\n",
       "      <td>-2.156969</td>\n",
       "      <td>-0.803986</td>\n",
       "      <td>-1.010804</td>\n",
       "      <td>-0.608947</td>\n",
       "      <td>-0.554969</td>\n",
       "      <td>-0.431636</td>\n",
       "      <td>-0.565726</td>\n",
       "      <td>-1.589062</td>\n",
       "      <td>-0.806676</td>\n",
       "      <td>-3.006156</td>\n",
       "      <td>-0.324849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.465180</td>\n",
       "      <td>-0.307403</td>\n",
       "      <td>-0.680554</td>\n",
       "      <td>-0.528478</td>\n",
       "      <td>-0.616386</td>\n",
       "      <td>-0.699287</td>\n",
       "      <td>-0.400153</td>\n",
       "      <td>-0.307467</td>\n",
       "      <td>-0.503195</td>\n",
       "      <td>-0.748928</td>\n",
       "      <td>-0.737477</td>\n",
       "      <td>-0.786234</td>\n",
       "      <td>-0.608947</td>\n",
       "      <td>-0.554969</td>\n",
       "      <td>-0.399955</td>\n",
       "      <td>-0.565726</td>\n",
       "      <td>-1.087599</td>\n",
       "      <td>-0.806676</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>-0.324849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.465180</td>\n",
       "      <td>-0.234920</td>\n",
       "      <td>-0.390242</td>\n",
       "      <td>-0.220376</td>\n",
       "      <td>-0.572577</td>\n",
       "      <td>-0.640374</td>\n",
       "      <td>-0.318782</td>\n",
       "      <td>-0.234966</td>\n",
       "      <td>-0.503195</td>\n",
       "      <td>0.659152</td>\n",
       "      <td>-0.652612</td>\n",
       "      <td>-0.446487</td>\n",
       "      <td>-0.088381</td>\n",
       "      <td>-0.195242</td>\n",
       "      <td>-0.318624</td>\n",
       "      <td>-0.218050</td>\n",
       "      <td>0.421275</td>\n",
       "      <td>-0.405464</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>-0.324849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.178299</td>\n",
       "      <td>-0.019391</td>\n",
       "      <td>0.273329</td>\n",
       "      <td>0.354082</td>\n",
       "      <td>0.306509</td>\n",
       "      <td>0.831346</td>\n",
       "      <td>-0.004259</td>\n",
       "      <td>-0.019273</td>\n",
       "      <td>0.066968</td>\n",
       "      <td>0.659152</td>\n",
       "      <td>1.027719</td>\n",
       "      <td>0.686001</td>\n",
       "      <td>0.432185</td>\n",
       "      <td>0.164486</td>\n",
       "      <td>-0.003929</td>\n",
       "      <td>0.129626</td>\n",
       "      <td>0.935020</td>\n",
       "      <td>0.396962</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>-0.324849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.094246</td>\n",
       "      <td>21.831881</td>\n",
       "      <td>2.927610</td>\n",
       "      <td>11.363510</td>\n",
       "      <td>2.689572</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>16.387194</td>\n",
       "      <td>21.831284</td>\n",
       "      <td>10.690830</td>\n",
       "      <td>0.659152</td>\n",
       "      <td>1.638749</td>\n",
       "      <td>2.384733</td>\n",
       "      <td>10.002362</td>\n",
       "      <td>10.226143</td>\n",
       "      <td>16.406502</td>\n",
       "      <td>10.240323</td>\n",
       "      <td>0.935020</td>\n",
       "      <td>8.822426</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>11.536596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_unique_count_for_card_state_1  Card_Merchdesc_State_total_7  \\\n",
       "count                           85264.000000                  85264.000000   \n",
       "mean                                0.004006                      0.002794   \n",
       "std                                 1.007299                      1.029148   \n",
       "min                                -0.465180                     -0.334628   \n",
       "25%                                -0.465180                     -0.307403   \n",
       "50%                                -0.465180                     -0.234920   \n",
       "75%                                 0.178299                     -0.019391   \n",
       "max                                10.094246                     21.831881   \n",
       "\n",
       "       Cardnum_count_1_by_30  Cardnum_max_14  Card_dow_vdratio_0by60  \\\n",
       "count           85264.000000    85264.000000            85264.000000   \n",
       "mean               -0.013411        0.003866                0.000122   \n",
       "std                 0.996083        0.988241                0.999618   \n",
       "min                -1.044444       -0.719426               -0.645300   \n",
       "25%                -0.680554       -0.528478               -0.616386   \n",
       "50%                -0.390242       -0.220376               -0.572577   \n",
       "75%                 0.273329        0.354082                0.306509   \n",
       "max                 2.927610       11.363510                2.689572   \n",
       "\n",
       "       Card_dow_vdratio_0by14  Merchnum_desc_State_total_3  \\\n",
       "count            85264.000000                 85264.000000   \n",
       "mean                 0.003494                     0.011369   \n",
       "std                  1.000623                     1.038863   \n",
       "min                 -0.759504                    -0.431853   \n",
       "25%                 -0.699287                    -0.400153   \n",
       "50%                 -0.640374                    -0.318782   \n",
       "75%                  0.831346                    -0.004259   \n",
       "max                  1.892914                    16.387194   \n",
       "\n",
       "       Card_Merchdesc_total_7  Card_dow_unique_count_for_merch_zip_7  \\\n",
       "count            85264.000000                           85264.000000   \n",
       "mean                 0.002805                               0.004227   \n",
       "std                  1.029151                               1.014974   \n",
       "min                 -0.334696                              -0.503195   \n",
       "25%                 -0.307467                              -0.503195   \n",
       "50%                 -0.234966                              -0.503195   \n",
       "75%                 -0.019273                               0.066968   \n",
       "max                 21.831284                              10.690830   \n",
       "\n",
       "       Cardnum_actual/toal_0  Card_dow_vdratio_0by7  Cardnum_vdratio_1by7  \\\n",
       "count           85264.000000           85264.000000          85264.000000   \n",
       "mean               -0.007076               0.004760             -0.000502   \n",
       "std                 1.003383               1.000962              0.995805   \n",
       "min                -2.156969              -0.803986             -1.010804   \n",
       "25%                -0.748928              -0.737477             -0.786234   \n",
       "50%                 0.659152              -0.652612             -0.446487   \n",
       "75%                 0.659152               1.027719              0.686001   \n",
       "max                 0.659152               1.638749              2.384733   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_3  \\\n",
       "count                           85264.000000   \n",
       "mean                                0.006726   \n",
       "std                                 1.007166   \n",
       "min                                -0.608947   \n",
       "25%                                -0.608947   \n",
       "50%                                -0.088381   \n",
       "75%                                 0.432185   \n",
       "max                                10.002362   \n",
       "\n",
       "       Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "count                         85264.000000               85264.000000   \n",
       "mean                              0.005786                   0.011402   \n",
       "std                               1.006367                   1.038809   \n",
       "min                              -0.554969                  -0.431636   \n",
       "25%                              -0.554969                  -0.399955   \n",
       "50%                              -0.195242                  -0.318624   \n",
       "75%                               0.164486                  -0.003929   \n",
       "max                              10.226143                  16.406502   \n",
       "\n",
       "       Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "count                         85264.000000           85264.000000   \n",
       "mean                              0.005656              -0.009701   \n",
       "std                               1.006149               1.001578   \n",
       "min                              -0.565726              -1.589062   \n",
       "25%                              -0.565726              -1.087599   \n",
       "50%                              -0.218050               0.421275   \n",
       "75%                               0.129626               0.935020   \n",
       "max                              10.240323               0.935020   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "count                           85264.000000          85264.000000   \n",
       "mean                                0.006369             -0.000725   \n",
       "std                                 1.009276              1.000119   \n",
       "min                                -0.806676             -3.006156   \n",
       "25%                                -0.806676              0.451857   \n",
       "50%                                -0.405464              0.451857   \n",
       "75%                                 0.396962              0.451857   \n",
       "max                                 8.822426              0.451857   \n",
       "\n",
       "       Card_dow_unique_count_for_merch_state_1  \n",
       "count                             85264.000000  \n",
       "mean                                  0.003383  \n",
       "std                                   1.013853  \n",
       "min                                  -0.324849  \n",
       "25%                                  -0.324849  \n",
       "50%                                  -0.324849  \n",
       "75%                                  -0.324849  \n",
       "max                                  11.536596  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trntst.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12232.000000</td>\n",
       "      <td>12232.000000</td>\n",
       "      <td>12232.000000</td>\n",
       "      <td>12232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.139962</td>\n",
       "      <td>-0.080014</td>\n",
       "      <td>-0.074155</td>\n",
       "      <td>-0.096704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.703259</td>\n",
       "      <td>1.454299</td>\n",
       "      <td>1.763518</td>\n",
       "      <td>1.083973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.525181</td>\n",
       "      <td>-7.156347</td>\n",
       "      <td>-5.583494</td>\n",
       "      <td>-5.793721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.193443</td>\n",
       "      <td>-0.695764</td>\n",
       "      <td>-0.617556</td>\n",
       "      <td>-0.544997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.189225</td>\n",
       "      <td>-0.390531</td>\n",
       "      <td>0.242040</td>\n",
       "      <td>0.046608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.472658</td>\n",
       "      <td>0.274310</td>\n",
       "      <td>0.769220</td>\n",
       "      <td>0.463284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.038502</td>\n",
       "      <td>36.498950</td>\n",
       "      <td>12.491464</td>\n",
       "      <td>6.578614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PC1           PC2           PC3           PC4\n",
       "count  12232.000000  12232.000000  12232.000000  12232.000000\n",
       "mean      -0.139962     -0.080014     -0.074155     -0.096704\n",
       "std        2.703259      1.454299      1.763518      1.083973\n",
       "min       -2.525181     -7.156347     -5.583494     -5.793721\n",
       "25%       -2.193443     -0.695764     -0.617556     -0.544997\n",
       "50%       -1.189225     -0.390531      0.242040      0.046608\n",
       "75%        1.472658      0.274310      0.769220      0.463284\n",
       "max       22.038502     36.498950     12.491464      6.578614"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_oot_orig_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# zscale the PCs\n",
    "mean = X_trntst_pca.mean()\n",
    "stdev = X_trntst_pca.std()\n",
    "X_trntst_pca = (X_trntst_pca - mean)/stdev\n",
    "X_oot_orig_pca = (X_oot_orig_pca - mean)/stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.526400e+04</td>\n",
       "      <td>8.526400e+04</td>\n",
       "      <td>8.526400e+04</td>\n",
       "      <td>8.526400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.066681e-17</td>\n",
       "      <td>1.083348e-17</td>\n",
       "      <td>-7.333430e-18</td>\n",
       "      <td>2.266697e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.196246e-01</td>\n",
       "      <td>-3.971647e+00</td>\n",
       "      <td>-2.953876e+00</td>\n",
       "      <td>-6.037210e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.542822e-01</td>\n",
       "      <td>-3.626125e-01</td>\n",
       "      <td>-3.988543e-01</td>\n",
       "      <td>-3.830293e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.285056e-01</td>\n",
       "      <td>-1.858258e-01</td>\n",
       "      <td>1.436538e-01</td>\n",
       "      <td>1.262343e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.405079e-01</td>\n",
       "      <td>1.635085e-01</td>\n",
       "      <td>4.700401e-01</td>\n",
       "      <td>5.400949e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.425471e+00</td>\n",
       "      <td>1.989997e+01</td>\n",
       "      <td>8.840546e+00</td>\n",
       "      <td>6.291476e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PC1           PC2           PC3           PC4\n",
       "count  8.526400e+04  8.526400e+04  8.526400e+04  8.526400e+04\n",
       "mean   1.066681e-17  1.083348e-17 -7.333430e-18  2.266697e-17\n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00\n",
       "min   -9.196246e-01 -3.971647e+00 -2.953876e+00 -6.037210e+00\n",
       "25%   -7.542822e-01 -3.626125e-01 -3.988543e-01 -3.830293e-01\n",
       "50%   -3.285056e-01 -1.858258e-01  1.436538e-01  1.262343e-01\n",
       "75%    5.405079e-01  1.635085e-01  4.700401e-01  5.400949e-01\n",
       "max    8.425471e+00  1.989997e+01  8.840546e+00  6.291476e+00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trntst_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12232.000000</td>\n",
       "      <td>12232.000000</td>\n",
       "      <td>12232.000000</td>\n",
       "      <td>12232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.050212</td>\n",
       "      <td>-0.041572</td>\n",
       "      <td>-0.039105</td>\n",
       "      <td>-0.089217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.969797</td>\n",
       "      <td>0.755593</td>\n",
       "      <td>0.929989</td>\n",
       "      <td>1.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.905911</td>\n",
       "      <td>-3.718140</td>\n",
       "      <td>-2.944447</td>\n",
       "      <td>-5.345207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.786900</td>\n",
       "      <td>-0.361490</td>\n",
       "      <td>-0.325667</td>\n",
       "      <td>-0.502806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.426635</td>\n",
       "      <td>-0.202903</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.142520</td>\n",
       "      <td>0.405647</td>\n",
       "      <td>0.427419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.906332</td>\n",
       "      <td>18.963335</td>\n",
       "      <td>6.587355</td>\n",
       "      <td>6.069339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PC1           PC2           PC3           PC4\n",
       "count  12232.000000  12232.000000  12232.000000  12232.000000\n",
       "mean      -0.050212     -0.041572     -0.039105     -0.089217\n",
       "std        0.969797      0.755593      0.929989      1.000059\n",
       "min       -0.905911     -3.718140     -2.944447     -5.345207\n",
       "25%       -0.786900     -0.361490     -0.325667     -0.502806\n",
       "50%       -0.426635     -0.202903      0.127639      0.043000\n",
       "75%        0.528317      0.142520      0.405647      0.427419\n",
       "max        7.906332     18.963335      6.587355      6.069339"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_oot_orig_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85264, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trntst_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12232, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_oot_orig_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample the larger class if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020524488647025708\n",
      "(3464, 20) 3464\n"
     ]
    }
   ],
   "source": [
    "# set the ratio of goods to bads that you would like. This next line is the ratio of goods to bads that you want for modeling\n",
    "sample_ratio_desired = 1\n",
    "\n",
    "temp = X_trntst.copy()\n",
    "temp['Fraud'] = Y_trntst['Fraud']\n",
    "temp.head()\n",
    "goods = temp[temp['Fraud']==0]\n",
    "bads = temp[temp['Fraud']==1]\n",
    "actual_bad_fraction = len(bads)/len(temp)\n",
    "actual_good_fraction = 1 - actual_bad_fraction\n",
    "print(actual_bad_fraction)\n",
    "fraction = sample_ratio_desired * actual_bad_fraction\n",
    "goods_sampled = goods.sample(frac = fraction)\n",
    "all_sampled = pd.concat([goods_sampled,bads])\n",
    "all_sampled.sort_index(inplace=True)\n",
    "Y_trntst_sampled = pd.DataFrame(all_sampled['Fraud'])\n",
    "X_trntst_sampled = all_sampled.drop(columns=['Fraud'])\n",
    "del [temp,goods,bads,all_sampled]\n",
    "gc.collect()\n",
    "print(X_trntst_sampled.shape,len(Y_trntst_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "niter = 0\n",
    "nitermax = 10\n",
    "jittersize = .1\n",
    "X_oot_orig = X_oot.copy()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can comment in/out any of these model cells and just explore one model type. You can also just rerun that single cell multiple times (hit shift-enter on that cell) as you manually explore different model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modeling_output = pd.DataFrame(columns=['Model','Trn','Tst','OOT'],index=range(1000))\n",
    "counter = 0\n",
    "model_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Logistic regression\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):    \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "#     model = LogisticRegression(penalty='elasticnet', C=1, solver='saga', l1_ratio=0.8)\n",
    " \n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['log reg',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7545526524148852 0.7186858316221766 0.5016835016835017\n",
      "1 0.7609756097560976 0.7192307692307692 0.46464646464646464\n",
      "2 0.7479608482871125 0.7385496183206107 0.45791245791245794\n",
      "3 0.7579462102689487 0.6998087954110899 0.49158249158249157\n",
      "4 0.7441485068603713 0.7377690802348337 0.468013468013468\n",
      "5 0.7473853580048271 0.7120315581854043 0.4781144781144781\n",
      "6 0.7557312252964427 0.7072164948453609 0.4781144781144781\n",
      "7 0.7463414634146341 0.725 0.4983164983164983\n",
      "8 0.7458432304038005 0.7453798767967146 0.4882154882154882\n",
      "9 0.760459392945037 0.7024482109227872 0.46464646464646464\n",
      "trn    0.752134\n",
      "tst    0.720612\n",
      "oot    0.479125\n",
      "dtype: float64\n",
      "CPU times: user 4.96 s, sys: 0 ns, total: 4.96 s\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Single DT\n",
    "\n",
    "FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = DecisionTreeClassifier(\n",
    "        #criterion='gini',\n",
    "        #splitter='best',  # or 'random'\n",
    "        max_depth=8,  # or an integer value\n",
    "        min_samples_split=120,  # or other integer based on the table\n",
    "        min_samples_leaf=60  # or other integer based on the table\n",
    "    )\n",
    " \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['DT',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "\n",
    "print(FDR.mean())\n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7707160096540627 0.7850098619329389 0.5353535353535354\n",
      "1 0.7864710676446618 0.7361376673040153 0.5420875420875421\n",
      "2 0.7955482275350371 0.7337057728119181 0.5016835016835017\n",
      "3 0.7893422148209825 0.761384335154827 0.5858585858585859\n",
      "4 0.7794238683127572 0.7663551401869159 0.5757575757575758\n",
      "5 0.7801131770412287 0.7758284600389863 0.5723905723905723\n",
      "6 0.7952105697770437 0.7532467532467533 0.5185185185185185\n",
      "7 0.7754271765663141 0.761996161228407 0.5824915824915825\n",
      "8 0.778496362166532 0.7699805068226121 0.5723905723905723\n",
      "9 0.7841004184100419 0.7675675675675676 0.5892255892255892\n",
      "trn    0.783485\n",
      "tst    0.761121\n",
      "oot    0.557576\n",
      "dtype: float64\n",
      "CPU times: user 1min 12s, sys: 0 ns, total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RF\n",
    "\n",
    "FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "# Note for students: The default values are a fairly complex architecture and make this cell run slow.\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=30,\n",
    "        #criterion='gini',\n",
    "        max_depth=20,\n",
    "        min_samples_split=120,\n",
    "        min_samples_leaf=60,\n",
    "        max_features=7\n",
    "    )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['RF',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "    \n",
    "print(FDR.mean())\n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1247, number of negative: 58437\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020893 -> initscore=-3.847209\n",
      "[LightGBM] [Info] Start training from score -3.847209\n",
      "0 0.756214915797915 0.7614314115308151 0.5050505050505051\n",
      "[LightGBM] [Info] Number of positive: 1218, number of negative: 58466\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020407 -> initscore=-3.871235\n",
      "[LightGBM] [Info] Start training from score -3.871235\n",
      "1 0.7479474548440066 0.7575187969924813 0.5555555555555556\n",
      "[LightGBM] [Info] Number of positive: 1241, number of negative: 58443\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3465\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020793 -> initscore=-3.852134\n",
      "[LightGBM] [Info] Start training from score -3.852134\n",
      "2 0.7606768734891217 0.762278978388998 0.531986531986532\n",
      "[LightGBM] [Info] Number of positive: 1207, number of negative: 58477\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020223 -> initscore=-3.880496\n",
      "[LightGBM] [Info] Start training from score -3.880496\n",
      "3 0.7630488815244407 0.7458563535911602 0.5589225589225589\n",
      "[LightGBM] [Info] Number of positive: 1206, number of negative: 58478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3464\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020206 -> initscore=-3.881342\n",
      "[LightGBM] [Info] Start training from score -3.881342\n",
      "4 0.7338308457711443 0.75 0.48148148148148145\n",
      "[LightGBM] [Info] Number of positive: 1211, number of negative: 58473\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3463\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020290 -> initscore=-3.877119\n",
      "[LightGBM] [Info] Start training from score -3.877119\n",
      "5 0.7646573080099092 0.7439703153988868 0.5555555555555556\n",
      "[LightGBM] [Info] Number of positive: 1217, number of negative: 58467\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3461\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020391 -> initscore=-3.872074\n",
      "[LightGBM] [Info] Start training from score -3.872074\n",
      "6 0.751848808545604 0.7598499061913696 0.5117845117845118\n",
      "[LightGBM] [Info] Number of positive: 1199, number of negative: 58485\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020089 -> initscore=-3.887282\n",
      "[LightGBM] [Info] Start training from score -3.887282\n",
      "7 0.7606338615512928 0.7078039927404719 0.5218855218855218\n",
      "[LightGBM] [Info] Number of positive: 1241, number of negative: 58443\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3463\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020793 -> initscore=-3.852134\n",
      "[LightGBM] [Info] Start training from score -3.852134\n",
      "8 0.7518130539887188 0.7367387033398821 0.531986531986532\n",
      "[LightGBM] [Info] Number of positive: 1201, number of negative: 58483\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020123 -> initscore=-3.885582\n",
      "[LightGBM] [Info] Start training from score -3.885582\n",
      "9 0.7568692756036636 0.7486338797814208 0.5589225589225589\n",
      "trn    0.754754\n",
      "tst    0.747408\n",
      "oot    0.531313\n",
      "dtype: float64\n",
      "CPU times: user 8.52 s, sys: 0 ns, total: 8.52 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LGBM\n",
    "\n",
    "FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "for niter in range(nitermax):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=30,\n",
    "        num_leaves = 4\n",
    "        #criterion='gini',\n",
    "        #max_depth=20,\n",
    "#         min_samples_split=1000,\n",
    "#         min_samples_leaf=100\n",
    "    )\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter,'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter,'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter,'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['LGBM',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "    \n",
    "print(FDR.mean())\n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # LGBM with SMOTE\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):    \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "    \n",
    "#     sm = SMOTE()\n",
    "    \n",
    "#     X_trn_sm, Y_trn_sm = sm.fit_resample(X_trn,Y_trn)\n",
    "    \n",
    "#     print(niter, X_trn.shape,Y_trn.shape)\n",
    "#     print(niter, X_trn_sm.shape,Y_trn_sm.shape)\n",
    "#     print(Y_trn.sum())\n",
    "#     print(Y_trn_sm.sum())\n",
    "\n",
    "#     model = lgb.LGBMClassifier()\n",
    "\n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn_sm, Y_trn_sm.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter,'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter,'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter,'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['LGBM with SMOTE',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # LGBM with jitter\n",
    "\n",
    "# jittersize = .1\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):    \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "    \n",
    "#     print(niter, X_trn.shape,Y_trn.shape)\n",
    "    \n",
    "#     X_trn_bads = X_trn[Y_trn == 1]\n",
    "#     Y_trn_bads = Y_trn[Y_trn == 1]\n",
    "    \n",
    "#     print(X_trn_bads.head())\n",
    "#     for i in range(2):\n",
    "#         X_trn_more = X_trn_bads*(1+jittersize*random.uniform(-1,1))\n",
    "#         X_trn = X_trn.append(X_trn_more,ignore_index=True)\n",
    "#         Y_trn = Y_trn.append(Y_trn_bads,ignore_index=True)\n",
    "        \n",
    "#     print(niter, X_trn.shape,Y_trn.shape)\n",
    "\n",
    "#     model = lgb.LGBMClassifier()\n",
    "\n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter,'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter,'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter,'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['LGBM with jitter',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8213689482470785 0.7554347826086957 0.569023569023569\n",
      "1 0.8050778050778051 0.7504725897920604 0.5252525252525253\n",
      "2 0.8287337662337663 0.777992277992278 0.5824915824915825\n",
      "3 0.8060897435897436 0.7928286852589641 0.5858585858585859\n",
      "4 0.8336079077429983 0.7985074626865671 0.5993265993265994\n",
      "5 0.8271103896103896 0.7895752895752896 0.5757575757575758\n",
      "6 0.8073770491803278 0.7660377358490567 0.5892255892255892\n",
      "7 0.8117744610281924 0.7830882352941176 0.5723905723905723\n",
      "8 0.8333333333333334 0.7724137931034483 0.5858585858585859\n",
      "9 0.8006535947712419 0.7699619771863118 0.5757575757575758\n",
      "trn    0.817513\n",
      "tst    0.775631\n",
      "oot    0.576094\n",
      "dtype: float64\n",
      "CPU times: user 1min 48s, sys: 40.9 s, total: 2min 29s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN\n",
    "\n",
    "FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "for niter in range(nitermax):  \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "    \n",
    "# Note for students: The default values are a fairly complex architecture and make this cell run slow.\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(20,20),\n",
    "        #activation='relu',\n",
    "        alpha=0.005,\n",
    "        #learning_rate='constant',\n",
    "        learning_rate_init=0.01,\n",
    "        #max_iter=50\n",
    "    )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['NN',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "    \n",
    "print(FDR.mean())\n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # NN on pc's\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst_pca, Y_trntst, test_size = .3)\n",
    "\n",
    "#     model = MLPClassifier(hidden_layer_sizes=(10,10),alpha=.005,solver='adam',activation='relu',\n",
    "#                           max_iter=1000,learning_rate='adaptive',learning_rate_init=.01)\n",
    "\n",
    "#     X_oot = X_oot_orig_pca.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['NN_PCs',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # GBC\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "#     model = GradientBoostingClassifier(learning_rate=0.01,max_depth=4,n_estimators=300)\n",
    "\n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['GBC',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7808 0.758 0.5353535353535354\n",
      "1 0.7905460472697636 0.7667304015296367 0.5521885521885522\n",
      "2 0.7922624053826746 0.7629233511586453 0.5218855218855218\n",
      "3 0.7822514379622021 0.7823639774859287 0.5185185185185185\n",
      "4 0.7824675324675324 0.7741312741312741 0.4781144781144781\n",
      "5 0.7723970944309927 0.8023483365949119 0.5084175084175084\n",
      "6 0.7899090157154673 0.722735674676525 0.5084175084175084\n",
      "7 0.788961038961039 0.7586872586872587 0.531986531986532\n",
      "8 0.7845394736842105 0.7790262172284644 0.5151515151515151\n",
      "9 0.7859450726978998 0.755859375 0.4983164983164983\n",
      "trn    0.785008\n",
      "tst    0.766281\n",
      "oot    0.516835\n",
      "dtype: float64\n",
      "CPU times: user 4min 22s, sys: 16.9 s, total: 4min 39s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Catboost\n",
    "\n",
    "FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "for niter in range(nitermax):  \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        verbose=0,\n",
    "        depth=3,\n",
    "        learning_rate=0.02,  # Ensure this is set correctly\n",
    "        l2_leaf_reg=6,\n",
    "        min_data_in_leaf=100\n",
    "    )\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['cat boost',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "    \n",
    "print(FDR.mean())\n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # NOTE this cell has been substantially modified to evaluate a sampled trn/tst data set. \n",
    "# # Only use this cell if you do downsampling of the goods.\n",
    "# # each good needs to have a weight of (1-actual_ratio)/sample_ratio_desired\n",
    "# # it's hard to get the correct FDR@3% for the actual train and test, so I just use the original trntst after the model is built for evaluation\n",
    "\n",
    "# xmult = actual_good_fraction / (actual_bad_fraction * sample_ratio_desired)\n",
    "# print(xmult)\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst_sampled, Y_trntst_sampled, test_size = .3)\n",
    "\n",
    "#     model = lgb.LGBMClassifier(n_estimators=50,\n",
    "#         criterion=gini,\n",
    "#         max_depth=None,\n",
    "#         min_samples_split=2,\n",
    "#         min_samples_leaf=1)\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())  \n",
    "    \n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn = X_trntst.copy()\n",
    "#     Y_trn = Y_trntst.copy()\n",
    "#     X_tst = X_trntst.copy()\n",
    "#     Y_tst = Y_trntst.copy()\n",
    "\n",
    "#     predictions = model.predict_proba(X_trntst)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trntst['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_trntst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_trntst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['LGBM sampled',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Catboost on pc's\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst_pca, Y_trntst, test_size = .3)\n",
    "\n",
    "#     model = CatBoostClassifier(verbose=0, iterations=100)\n",
    "# #             learning_rate=detect_rate,\n",
    "# #             l2_leaf_reg=5  \n",
    "# #\n",
    "\n",
    "#     X_oot = X_oot_orig_pca.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['cat boost_PCs',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # unsupervised model using pc's. \n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst_pca, Y_trntst, test_size = .3)\n",
    "\n",
    "#     X_oot = X_oot_orig_pca.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     pow = 2\n",
    "#     oop = 1/pow\n",
    "#     predictions = ((X_trn.abs()**pow).sum(axis=1))**oop\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = ((X_tst.abs()**pow).sum(axis=1))**oop\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = ((X_oot.abs()**pow).sum(axis=1))**oop\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['unsupervised outliers',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8058091286307054 0.7834862385321101 0.5454545454545454\n",
      "1 0.8115477145148356 0.7872763419483101 0.5555555555555556\n",
      "2 0.8244336569579288 0.7859922178988327 0.5488215488215489\n",
      "3 0.8127090301003345 0.7653429602888087 0.5016835016835017\n",
      "4 0.8137254901960784 0.7870722433460076 0.5420875420875421\n",
      "5 0.8098360655737705 0.8 0.5723905723905723\n",
      "6 0.8196457326892109 0.7874015748031497 0.5084175084175084\n",
      "7 0.8087027914614121 0.7857142857142857 0.5353535353535354\n",
      "8 0.8054635761589404 0.8025830258302583 0.5117845117845118\n",
      "9 0.8237232289950577 0.753731343283582 0.5016835016835017\n",
      "trn    0.813560\n",
      "tst    0.783860\n",
      "oot    0.532323\n",
      "dtype: float64\n",
      "CPU times: user 14 s, sys: 43.8 ms, total: 14.1 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGB\n",
    "\n",
    "FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "for niter in range(nitermax):  \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "#     model = xgb.XGBClassifier(booster='gbtree',max_depth=4,min_child_weight=75,gamma=0.01)\n",
    "    model = xgb.XGBClassifier(booster='gbtree',n_estimators=100,max_depth=3,learning_rate=.1)\n",
    "\n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['XGB',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "    \n",
    "print(FDR.mean())\n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Knn\n",
    "# # Knn can be very slow with a lot of records.\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "#     model = KNeighborsClassifier(n_neighbors=80) \n",
    "    \n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['Knn',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # SVM\n",
    "# # SVM can be very slow. It scales like the # training records cubed\n",
    "\n",
    "# FDR = pd.DataFrame(np.zeros((nitermax,3)), columns=('trn', 'tst', 'oot'))\n",
    "# for niter in range(nitermax):  \n",
    "#     X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "#     model = svm.SVC(kernel='poly',probability=True)\n",
    "    \n",
    "#     X_oot = X_oot_orig.copy()\n",
    "#     X_trn_save = X_trn.copy()\n",
    "#     Y_trn_save = Y_trn.copy()\n",
    "\n",
    "#     model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "#     predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "#     X_trn['predicted'] = predictions\n",
    "#     X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "#     topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "#     temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_tst)[:,1]\n",
    "#     X_tst['predicted']=predictions\n",
    "#     X_tst['Fraud'] = Y_tst['Fraud']\n",
    "#     topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "#     temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "#     predictions = model.predict_proba(X_oot)[:,1]\n",
    "#     X_oot['predicted']=predictions\n",
    "#     X_oot['Fraud'] = Y_oot['Fraud']\n",
    "#     topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "#     temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "#     needed = temp.loc[:,'Fraud']\n",
    "#     FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "#     print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "#     Modeling_output.iloc[counter] = ['SVM',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "#     counter = counter + 1\n",
    "    \n",
    "# print(FDR.mean())\n",
    "# model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Trn</th>\n",
       "      <th>Tst</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.754553</td>\n",
       "      <td>0.718686</td>\n",
       "      <td>0.501684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.719231</td>\n",
       "      <td>0.464646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.747961</td>\n",
       "      <td>0.73855</td>\n",
       "      <td>0.457912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.757946</td>\n",
       "      <td>0.699809</td>\n",
       "      <td>0.491582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.744149</td>\n",
       "      <td>0.737769</td>\n",
       "      <td>0.468013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.747385</td>\n",
       "      <td>0.712032</td>\n",
       "      <td>0.478114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.755731</td>\n",
       "      <td>0.707216</td>\n",
       "      <td>0.478114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.746341</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.498316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.745843</td>\n",
       "      <td>0.74538</td>\n",
       "      <td>0.488215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.760459</td>\n",
       "      <td>0.702448</td>\n",
       "      <td>0.464646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.770716</td>\n",
       "      <td>0.78501</td>\n",
       "      <td>0.535354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.786471</td>\n",
       "      <td>0.736138</td>\n",
       "      <td>0.542088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.795548</td>\n",
       "      <td>0.733706</td>\n",
       "      <td>0.501684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.789342</td>\n",
       "      <td>0.761384</td>\n",
       "      <td>0.585859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.779424</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.780113</td>\n",
       "      <td>0.775828</td>\n",
       "      <td>0.572391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.795211</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.775427</td>\n",
       "      <td>0.761996</td>\n",
       "      <td>0.582492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.778496</td>\n",
       "      <td>0.769981</td>\n",
       "      <td>0.572391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.7841</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.589226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model       Trn       Tst       OOT\n",
       "0     DT  0.754553  0.718686  0.501684\n",
       "1     DT  0.760976  0.719231  0.464646\n",
       "2     DT  0.747961   0.73855  0.457912\n",
       "3     DT  0.757946  0.699809  0.491582\n",
       "4     DT  0.744149  0.737769  0.468013\n",
       "5     DT  0.747385  0.712032  0.478114\n",
       "6     DT  0.755731  0.707216  0.478114\n",
       "7     DT  0.746341     0.725  0.498316\n",
       "8     DT  0.745843   0.74538  0.488215\n",
       "9     DT  0.760459  0.702448  0.464646\n",
       "10    RF  0.770716   0.78501  0.535354\n",
       "11    RF  0.786471  0.736138  0.542088\n",
       "12    RF  0.795548  0.733706  0.501684\n",
       "13    RF  0.789342  0.761384  0.585859\n",
       "14    RF  0.779424  0.766355  0.575758\n",
       "15    RF  0.780113  0.775828  0.572391\n",
       "16    RF  0.795211  0.753247  0.518519\n",
       "17    RF  0.775427  0.761996  0.582492\n",
       "18    RF  0.778496  0.769981  0.572391\n",
       "19    RF    0.7841  0.767568  0.589226"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Modeling_output.dropna()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.754553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.760976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.747961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.757946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.744149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Type     Value\n",
       "0    DT  Trn  0.754553\n",
       "1    DT  Trn  0.760976\n",
       "2    DT  Trn  0.747961\n",
       "3    DT  Trn  0.757946\n",
       "4    DT  Trn  0.744149"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unpivot = df.melt(id_vars='Model', value_vars=['Trn', 'Tst', 'OOT'], var_name='Type', value_name='Value')\n",
    "df_unpivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.754553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.760976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.747961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.757946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>Trn</td>\n",
       "      <td>0.744149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Type     Value\n",
       "0    DT  Trn  0.754553\n",
       "1    DT  Trn  0.760976\n",
       "2    DT  Trn  0.747961\n",
       "3    DT  Trn  0.757946\n",
       "4    DT  Trn  0.744149"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare = df_unpivot[(df_unpivot['Type']=='Trn') | (df_unpivot['Type']=='Tst') | (df_unpivot['Type']=='OOT')]\n",
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Trn</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tst</th>\n",
       "      <th colspan=\"2\" halign=\"left\">OOT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0.752134</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.720612</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.479125</td>\n",
       "      <td>0.015311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.754754</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.747408</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.531313</td>\n",
       "      <td>0.026550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.817513</td>\n",
       "      <td>0.012681</td>\n",
       "      <td>0.775631</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>0.576094</td>\n",
       "      <td>0.019979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.783485</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>0.761121</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.557576</td>\n",
       "      <td>0.030867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.81356</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.78386</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>0.532323</td>\n",
       "      <td>0.024867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat boost</th>\n",
       "      <td>0.785008</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>0.766281</td>\n",
       "      <td>0.020886</td>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.020649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Trn                 Tst                 OOT          \n",
       "               mean       std      mean       std      mean       std\n",
       "Model                                                                \n",
       "DT         0.752134  0.006471  0.720612  0.015882  0.479125  0.015311\n",
       "LGBM       0.754754  0.009094  0.747408  0.016260  0.531313  0.026550\n",
       "NN         0.817513  0.012681  0.775631  0.015812  0.576094  0.019979\n",
       "RF         0.783485  0.008215  0.761121  0.016238  0.557576  0.030867\n",
       "XGB         0.81356  0.006884   0.78386  0.014546  0.532323  0.024867\n",
       "cat boost  0.785008  0.005915  0.766281  0.020886  0.516835  0.020649"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = df.groupby('Model').agg({'Trn':['mean','std'],'Tst':['mean','std'],'OOT':['mean','std']})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Trn</th>\n",
       "      <th>Tst</th>\n",
       "      <th>OOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.754553</td>\n",
       "      <td>0.718686</td>\n",
       "      <td>0.501684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.719231</td>\n",
       "      <td>0.464646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.747961</td>\n",
       "      <td>0.73855</td>\n",
       "      <td>0.457912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.757946</td>\n",
       "      <td>0.699809</td>\n",
       "      <td>0.491582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.744149</td>\n",
       "      <td>0.737769</td>\n",
       "      <td>0.468013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model       Trn       Tst       OOT\n",
       "0    DT  0.754553  0.718686  0.501684\n",
       "1    DT  0.760976  0.719231  0.464646\n",
       "2    DT  0.747961   0.73855  0.457912\n",
       "3    DT  0.757946  0.699809  0.491582\n",
       "4    DT  0.744149  0.737769  0.468013"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5993265993265994"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_OOT = df['OOT'].max()\n",
    "best_OOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))  # Adjust figure size as needed\n",
    "ax = sns.boxplot(\n",
    "    x='Model',\n",
    "    y='Value',\n",
    "    hue='Type',\n",
    "    data=df_compare,\n",
    "    palette='tab10',  # This uses a predefined palette that's well-differentiated\n",
    "    width=0.6  # Adjust the width of the boxes to prevent overlap\n",
    ")\n",
    "\n",
    "# Set the limits of the y-axis to improve spacing\n",
    "plt.ylim(0.4, 0.85)\n",
    "\n",
    "# Enhancing the font sizes for better readability\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score (FDR%)', fontsize=14)\n",
    "plt.title('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "plt.xticks(rotation=45, fontsize=12)  # Rotate x labels for better legibility\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(title='Type', fontsize=12, title_fontsize='13')  # Adjust legend\n",
    "\n",
    "plt.grid(axis='y')  # Add gridlines\n",
    "\n",
    "plt.tight_layout()  # Adjust subplot parameters to give some padding\n",
    "plt.savefig('modeling.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  0:03:28.579601\n"
     ]
    }
   ],
   "source": [
    "print('duration: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell can be used to explore overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1246, number of negative: 58438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3459\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020877 -> initscore=-3.848028\n",
      "[LightGBM] [Info] Start training from score -3.848028\n",
      "0 0.723916532905297 0.7003968253968254 0.4882154882154882\n",
      "[LightGBM] [Info] Number of positive: 1244, number of negative: 58440\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3468\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020843 -> initscore=-3.849669\n",
      "[LightGBM] [Info] Start training from score -3.849669\n",
      "1 0.7218649517684887 0.7233201581027668 0.48148148148148145\n",
      "[LightGBM] [Info] Number of positive: 1209, number of negative: 58475\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020257 -> initscore=-3.878806\n",
      "[LightGBM] [Info] Start training from score -3.878806\n",
      "2 0.7113316790736146 0.7430683918669131 0.4882154882154882\n",
      "loop trn tst oot 2 0.7190377212491335 0.7222617917888351 0.4859708193041527\n",
      "[LightGBM] [Info] Number of positive: 1229, number of negative: 58455\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020592 -> initscore=-3.862056\n",
      "[LightGBM] [Info] Start training from score -3.862056\n",
      "0 0.7851912123677787 0.7485604606525912 0.5420875420875421\n",
      "[LightGBM] [Info] Number of positive: 1223, number of negative: 58461\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3469\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020491 -> initscore=-3.867053\n",
      "[LightGBM] [Info] Start training from score -3.867053\n",
      "1 0.7849550286181521 0.7324478178368121 0.4983164983164983\n",
      "[LightGBM] [Info] Number of positive: 1243, number of negative: 58441\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3468\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020826 -> initscore=-3.850490\n",
      "[LightGBM] [Info] Start training from score -3.850490\n",
      "2 0.7779565567176187 0.7652859960552268 0.5488215488215489\n",
      "loop trn tst oot 3 0.7827009325678498 0.7487647581815433 0.5297418630751964\n",
      "[LightGBM] [Info] Number of positive: 1210, number of negative: 58474\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3464\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020273 -> initscore=-3.877962\n",
      "[LightGBM] [Info] Start training from score -3.877962\n",
      "0 0.8264462809917356 0.75 0.5791245791245792\n",
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 58469\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3464\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020357 -> initscore=-3.873753\n",
      "[LightGBM] [Info] Start training from score -3.873753\n",
      "1 0.8213991769547325 0.7626168224299066 0.569023569023569\n",
      "[LightGBM] [Info] Number of positive: 1223, number of negative: 58461\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020491 -> initscore=-3.867053\n",
      "[LightGBM] [Info] Start training from score -3.867053\n",
      "2 0.8094848732624693 0.7552182163187856 0.5286195286195287\n",
      "loop trn tst oot 4 0.8191101104029791 0.7559450129162307 0.5589225589225589\n",
      "[LightGBM] [Info] Number of positive: 1220, number of negative: 58464\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3465\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020441 -> initscore=-3.869560\n",
      "[LightGBM] [Info] Start training from score -3.869560\n",
      "0 0.8442622950819673 0.7924528301886793 0.5555555555555556\n",
      "[LightGBM] [Info] Number of positive: 1212, number of negative: 58472\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3461\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020307 -> initscore=-3.876276\n",
      "[LightGBM] [Info] Start training from score -3.876276\n",
      "1 0.8168316831683168 0.7881040892193308 0.569023569023569\n",
      "[LightGBM] [Info] Number of positive: 1221, number of negative: 58463\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3464\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020458 -> initscore=-3.868724\n",
      "[LightGBM] [Info] Start training from score -3.868724\n",
      "2 0.8419328419328419 0.7618147448015122 0.531986531986532\n",
      "loop trn tst oot 5 0.8343422733943754 0.7807905547365075 0.5521885521885522\n",
      "[LightGBM] [Info] Number of positive: 1249, number of negative: 58435\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3464\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020927 -> initscore=-3.845572\n",
      "[LightGBM] [Info] Start training from score -3.845572\n",
      "0 0.8486789431545236 0.8143712574850299 0.5656565656565656\n",
      "[LightGBM] [Info] Number of positive: 1190, number of negative: 58494\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3465\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019938 -> initscore=-3.894971\n",
      "[LightGBM] [Info] Start training from score -3.894971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8554621848739495 0.7892857142857143 0.5488215488215489\n",
      "[LightGBM] [Info] Number of positive: 1199, number of negative: 58485\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3468\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020089 -> initscore=-3.887282\n",
      "[LightGBM] [Info] Start training from score -3.887282\n",
      "2 0.8673894912427023 0.79491833030853 0.531986531986532\n",
      "loop trn tst oot 6 0.8571768730903918 0.7995251006930914 0.5488215488215489\n",
      "[LightGBM] [Info] Number of positive: 1191, number of negative: 58493\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3468\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019955 -> initscore=-3.894114\n",
      "[LightGBM] [Info] Start training from score -3.894114\n",
      "0 0.9026028547439127 0.7549194991055456 0.5858585858585859\n",
      "[LightGBM] [Info] Number of positive: 1242, number of negative: 58442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3472\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020810 -> initscore=-3.851312\n",
      "[LightGBM] [Info] Start training from score -3.851312\n",
      "1 0.8695652173913043 0.7736220472440944 0.5252525252525253\n",
      "[LightGBM] [Info] Number of positive: 1233, number of negative: 58451\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3463\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020659 -> initscore=-3.858739\n",
      "[LightGBM] [Info] Start training from score -3.858739\n",
      "2 0.8807785888077859 0.7969052224371374 0.5252525252525253\n",
      "loop trn tst oot 7 0.8843155536476677 0.7751489229289258 0.5454545454545455\n",
      "[LightGBM] [Info] Number of positive: 1214, number of negative: 58470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3462\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020340 -> initscore=-3.874593\n",
      "[LightGBM] [Info] Start training from score -3.874593\n",
      "0 0.8986820428336079 0.7667910447761194 0.5824915824915825\n",
      "[LightGBM] [Info] Number of positive: 1248, number of negative: 58436\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020910 -> initscore=-3.846390\n",
      "[LightGBM] [Info] Start training from score -3.846390\n",
      "1 0.8910256410256411 0.8187250996015937 0.531986531986532\n",
      "[LightGBM] [Info] Number of positive: 1203, number of negative: 58481\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3463\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020156 -> initscore=-3.883883\n",
      "[LightGBM] [Info] Start training from score -3.883883\n",
      "2 0.8869492934330839 0.7696526508226691 0.5252525252525253\n",
      "loop trn tst oot 8 0.8922189924307776 0.7850562650667939 0.5465768799102133\n",
      "[LightGBM] [Info] Number of positive: 1203, number of negative: 58481\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3467\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020156 -> initscore=-3.883883\n",
      "[LightGBM] [Info] Start training from score -3.883883\n",
      "0 0.9177057356608479 0.8080438756855576 0.5521885521885522\n",
      "[LightGBM] [Info] Number of positive: 1219, number of negative: 58465\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3466\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020424 -> initscore=-3.870397\n",
      "[LightGBM] [Info] Start training from score -3.870397\n",
      "1 0.9048400328137818 0.8267419962335216 0.5084175084175084\n",
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 58469\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3464\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020357 -> initscore=-3.873753\n",
      "[LightGBM] [Info] Start training from score -3.873753\n",
      "2 0.9135802469135802 0.8280373831775701 0.5589225589225589\n",
      "loop trn tst oot 9 0.9120420051294033 0.8209410850322164 0.5398428731762065\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 230 ms, total: 1min 4s\n",
      "Wall time: 9.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nitermax2 = 3\n",
    "training = []\n",
    "testing = []\n",
    "oot = []\n",
    "results = pd.DataFrame(np.zeros((niter,3)),columns=['trn','tst','oot'])\n",
    "for i in range(2,10,1):\n",
    "    for niter in range(nitermax2):\n",
    "        X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "        \n",
    "        model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        num_leaves = i\n",
    "        )\n",
    "\n",
    "        model.fit(X_trn, Y_trn.values.ravel()) \n",
    "\n",
    "        X_oot = X_oot_orig.copy()\n",
    "        X_trn_save = X_trn.copy()\n",
    "        Y_trn_save = Y_trn.copy()\n",
    "    \n",
    "        predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "        X_trn['predicted'] = predictions\n",
    "        X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "        topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "        temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "        needed = temp.loc[:,'Fraud']\n",
    "        results.loc[niter,'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "        predictions = model.predict_proba(X_tst)[:,1]\n",
    "        X_tst['predicted']=predictions\n",
    "        X_tst['Fraud'] = Y_tst['Fraud']\n",
    "        topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "        temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "        needed = temp.loc[:,'Fraud']\n",
    "        results.loc[niter,'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "        predictions = model.predict_proba(X_oot)[:,1]\n",
    "        X_oot['predicted']=predictions\n",
    "        X_oot['Fraud'] = Y_oot['Fraud']\n",
    "        topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "        temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "        needed = temp.loc[:,'Fraud']\n",
    "        results.loc[niter,'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "        print(niter, results.loc[niter,'trn'],results.loc[niter,'tst'],results.loc[niter,'oot'],)\n",
    "\n",
    "    results_mean_trn = results['trn'].mean()\n",
    "    results_mean_tst = results['tst'].mean()\n",
    "    results_mean_oot = results['oot'].mean()\n",
    "    print('loop', 'trn', 'tst', 'oot', i, results_mean_trn, results_mean_tst, results_mean_oot)\n",
    "    training.append(results_mean_trn)\n",
    "    testing.append(results_mean_tst)\n",
    "    oot.append(results_mean_oot)\n",
    "\n",
    "table=pd.DataFrame({'n': range(1,len(training)+1),'training':training,'testing':testing,'oot':oot})\n",
    "table.set_index('n',inplace=True) \n",
    "table.plot()\n",
    "plt.xlabel('num_leaves', fontsize=12)\n",
    "plt.ylabel('Score (FDR%)', fontsize=12)\n",
    "plt.title('Effect of num_leaves on Model Performance', fontsize=14)\n",
    "#plt.savefig('complexity_NN.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5993265993265994"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_OOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook makes the tables for your final model of choice. You need to run that final model only once (no CV). If you want you can run the below cell over and over by itself until it gives you a model you like (due to the stochastic nature of some ML algorithms, but you can't change from your best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desire = 0.59\n",
      "[LightGBM] [Info] Number of positive: 1229, number of negative: 58455\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3470\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020592 -> initscore=-3.862056\n",
      "[LightGBM] [Info] Start training from score -3.862056\n",
      "0 0.806346623270952 0.7715930902111324 0.5521885521885522\n",
      "[LightGBM] [Info] Number of positive: 1220, number of negative: 58464\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3468\n",
      "[LightGBM] [Info] Number of data points in the train set: 59684, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020441 -> initscore=-3.869560\n",
      "[LightGBM] [Info] Start training from score -3.869560\n",
      "1 0.7934426229508197 0.7849056603773585 0.5925925925925926\n",
      "CPU times: user 5.36 s, sys: 37.3 ms, total: 5.4 s\n",
      "Wall time: 818 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Note - need to set this next value to around the highest oot fdr for your model of choice.\n",
    "# The model then runs a nombermof times with your fixed hyperparameters until it finds a good model.\n",
    "desire = .59\n",
    "print('desire =', desire)\n",
    "for niter in range(50):    \n",
    "    X_trn, X_tst, Y_trn, Y_tst = train_test_split(X_trntst, Y_trntst, test_size = .3)\n",
    "\n",
    "# here's where you put your final model of choice\n",
    "# I run this loop a large number of times with an unreasonably high stopping criterion (the break condition)\n",
    "# I then look at all these runs and select a value of the oot performance where I want to break out this loop\n",
    "# and that will be my final model run of choice\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        num_leaves = 4\n",
    "        )\n",
    "    \n",
    "    X_oot = X_oot_orig.copy()\n",
    "    X_trn_save = X_trn.copy()\n",
    "    Y_trn_save = Y_trn.copy()\n",
    "\n",
    "    model.fit(X_trn, Y_trn.values.ravel())   \n",
    "\n",
    "    predictions = model.predict_proba(X_trn_save)[:,1]\n",
    "    X_trn['predicted'] = predictions\n",
    "    X_trn['Fraud'] = Y_trn_save['Fraud']\n",
    "    topRows = int(round(X_trn.shape[0]*detect_rate))\n",
    "    temp = X_trn.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'trn'] = sum(needed)/sum(X_trn.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_tst)[:,1]\n",
    "    X_tst['predicted']=predictions\n",
    "    X_tst['Fraud'] = Y_tst['Fraud']\n",
    "    topRows = int(round(X_tst.shape[0]*detect_rate))\n",
    "    temp = X_tst.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'tst'] = sum(needed)/sum(X_tst.loc[:,'Fraud'])\n",
    "\n",
    "    predictions = model.predict_proba(X_oot)[:,1]\n",
    "    X_oot['predicted']=predictions\n",
    "    X_oot['Fraud'] = Y_oot['Fraud']\n",
    "    topRows = int(round(X_oot.shape[0]*detect_rate))\n",
    "    temp = X_oot.sort_values('predicted',ascending=False).head(topRows)\n",
    "    needed = temp.loc[:,'Fraud']\n",
    "    FDR.loc[niter, 'oot'] = sum(needed)/sum(X_oot.loc[:,'Fraud'])\n",
    "    print(niter, FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot'])\n",
    "    Modeling_output.iloc[counter] = ['LGBM',FDR.loc[niter, 'trn'],FDR.loc[niter, 'tst'],FDR.loc[niter, 'oot']]\n",
    "    counter = counter + 1\n",
    "# choose a good break point\n",
    "    if(FDR.loc[niter, 'oot'] > desire): break\n",
    "    \n",
    "model_counter = model_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn_eval = X_trn.copy()\n",
    "X_tst_eval = X_tst.copy()\n",
    "X_oot_eval = X_oot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardnum_unique_count_for_card_state_1</th>\n",
       "      <th>Card_Merchdesc_State_total_7</th>\n",
       "      <th>Cardnum_count_1_by_30</th>\n",
       "      <th>Cardnum_max_14</th>\n",
       "      <th>Card_dow_vdratio_0by60</th>\n",
       "      <th>Card_dow_vdratio_0by14</th>\n",
       "      <th>Merchnum_desc_State_total_3</th>\n",
       "      <th>Card_Merchdesc_total_7</th>\n",
       "      <th>Card_dow_unique_count_for_merch_zip_7</th>\n",
       "      <th>Cardnum_actual/toal_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_3</th>\n",
       "      <th>Cardnum_unique_count_for_card_zip_3</th>\n",
       "      <th>Merchnum_desc_Zip_total_3</th>\n",
       "      <th>Cardnum_unique_count_for_Merchnum_3</th>\n",
       "      <th>Cardnum_actual/toal_1</th>\n",
       "      <th>Cardnum_unique_count_for_card_state_7</th>\n",
       "      <th>Cardnum_actual/max_0</th>\n",
       "      <th>Card_dow_unique_count_for_merch_state_1</th>\n",
       "      <th>predicted</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90933</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>-0.046803</td>\n",
       "      <td>2.190310</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.493320</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.260007</td>\n",
       "      <td>-0.046880</td>\n",
       "      <td>7.479088</td>\n",
       "      <td>-1.774862</td>\n",
       "      <td>...</td>\n",
       "      <td>5.637844</td>\n",
       "      <td>5.920128</td>\n",
       "      <td>-0.259483</td>\n",
       "      <td>5.692441</td>\n",
       "      <td>-1.325927</td>\n",
       "      <td>4.007875</td>\n",
       "      <td>-0.877403</td>\n",
       "      <td>10.287893</td>\n",
       "      <td>0.990481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94031</th>\n",
       "      <td>7.900037</td>\n",
       "      <td>-0.324574</td>\n",
       "      <td>1.047494</td>\n",
       "      <td>-0.114619</td>\n",
       "      <td>1.512059</td>\n",
       "      <td>1.671754</td>\n",
       "      <td>-0.425850</td>\n",
       "      <td>-0.324642</td>\n",
       "      <td>5.198436</td>\n",
       "      <td>-2.142262</td>\n",
       "      <td>...</td>\n",
       "      <td>6.158410</td>\n",
       "      <td>5.560400</td>\n",
       "      <td>-0.425622</td>\n",
       "      <td>5.692441</td>\n",
       "      <td>-1.579579</td>\n",
       "      <td>4.409088</td>\n",
       "      <td>-2.940496</td>\n",
       "      <td>7.255681</td>\n",
       "      <td>0.980239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88021</th>\n",
       "      <td>5.326124</td>\n",
       "      <td>0.029211</td>\n",
       "      <td>1.479820</td>\n",
       "      <td>-0.266383</td>\n",
       "      <td>2.432935</td>\n",
       "      <td>1.688766</td>\n",
       "      <td>-0.214622</td>\n",
       "      <td>0.029132</td>\n",
       "      <td>5.198436</td>\n",
       "      <td>-1.344831</td>\n",
       "      <td>...</td>\n",
       "      <td>4.076147</td>\n",
       "      <td>3.761762</td>\n",
       "      <td>-0.214017</td>\n",
       "      <td>3.606385</td>\n",
       "      <td>-0.886161</td>\n",
       "      <td>3.205450</td>\n",
       "      <td>0.166403</td>\n",
       "      <td>7.255681</td>\n",
       "      <td>0.978587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94135</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>-0.082879</td>\n",
       "      <td>0.557716</td>\n",
       "      <td>-0.114619</td>\n",
       "      <td>2.212960</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.281546</td>\n",
       "      <td>-0.082955</td>\n",
       "      <td>2.347620</td>\n",
       "      <td>-1.374680</td>\n",
       "      <td>...</td>\n",
       "      <td>7.199542</td>\n",
       "      <td>7.359038</td>\n",
       "      <td>-0.281060</td>\n",
       "      <td>7.778496</td>\n",
       "      <td>-1.364339</td>\n",
       "      <td>5.211513</td>\n",
       "      <td>-0.433736</td>\n",
       "      <td>3.465416</td>\n",
       "      <td>0.977696</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93866</th>\n",
       "      <td>4.682646</td>\n",
       "      <td>0.086734</td>\n",
       "      <td>0.671471</td>\n",
       "      <td>-0.122573</td>\n",
       "      <td>0.869779</td>\n",
       "      <td>1.450594</td>\n",
       "      <td>-0.180278</td>\n",
       "      <td>0.086652</td>\n",
       "      <td>1.777457</td>\n",
       "      <td>-1.044678</td>\n",
       "      <td>...</td>\n",
       "      <td>3.555581</td>\n",
       "      <td>3.402034</td>\n",
       "      <td>-0.179612</td>\n",
       "      <td>3.606385</td>\n",
       "      <td>-1.002588</td>\n",
       "      <td>2.403025</td>\n",
       "      <td>-0.217180</td>\n",
       "      <td>2.707363</td>\n",
       "      <td>0.976888</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96243</th>\n",
       "      <td>7.900037</td>\n",
       "      <td>-0.322862</td>\n",
       "      <td>2.330397</td>\n",
       "      <td>-0.188352</td>\n",
       "      <td>2.689572</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.424828</td>\n",
       "      <td>-0.322929</td>\n",
       "      <td>5.198436</td>\n",
       "      <td>-2.137171</td>\n",
       "      <td>...</td>\n",
       "      <td>6.158410</td>\n",
       "      <td>5.560400</td>\n",
       "      <td>-0.424598</td>\n",
       "      <td>5.344765</td>\n",
       "      <td>-1.575458</td>\n",
       "      <td>4.409088</td>\n",
       "      <td>-2.918637</td>\n",
       "      <td>7.255681</td>\n",
       "      <td>0.973769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90115</th>\n",
       "      <td>-0.465180</td>\n",
       "      <td>12.690366</td>\n",
       "      <td>2.774478</td>\n",
       "      <td>1.493632</td>\n",
       "      <td>2.689572</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>7.344743</td>\n",
       "      <td>12.689867</td>\n",
       "      <td>-0.503195</td>\n",
       "      <td>-1.493364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.608947</td>\n",
       "      <td>-0.554969</td>\n",
       "      <td>7.358863</td>\n",
       "      <td>-0.565726</td>\n",
       "      <td>-1.449263</td>\n",
       "      <td>-0.806676</td>\n",
       "      <td>-0.303934</td>\n",
       "      <td>-0.324849</td>\n",
       "      <td>0.972978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95997</th>\n",
       "      <td>4.039168</td>\n",
       "      <td>-0.113518</td>\n",
       "      <td>2.008820</td>\n",
       "      <td>-0.380382</td>\n",
       "      <td>2.689572</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.299839</td>\n",
       "      <td>-0.113593</td>\n",
       "      <td>1.207294</td>\n",
       "      <td>-1.345079</td>\n",
       "      <td>...</td>\n",
       "      <td>3.035015</td>\n",
       "      <td>3.042307</td>\n",
       "      <td>-0.299386</td>\n",
       "      <td>2.911033</td>\n",
       "      <td>-1.151123</td>\n",
       "      <td>2.001812</td>\n",
       "      <td>-0.248524</td>\n",
       "      <td>1.949310</td>\n",
       "      <td>0.972401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96225</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>0.130242</td>\n",
       "      <td>2.298964</td>\n",
       "      <td>-0.188352</td>\n",
       "      <td>2.689572</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>0.837749</td>\n",
       "      <td>0.130159</td>\n",
       "      <td>4.628273</td>\n",
       "      <td>-1.368060</td>\n",
       "      <td>...</td>\n",
       "      <td>5.637844</td>\n",
       "      <td>5.200672</td>\n",
       "      <td>0.840236</td>\n",
       "      <td>4.997089</td>\n",
       "      <td>-1.047553</td>\n",
       "      <td>4.007875</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>6.497628</td>\n",
       "      <td>0.971630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90973</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>-0.328159</td>\n",
       "      <td>2.264040</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.522758</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.427990</td>\n",
       "      <td>-0.328227</td>\n",
       "      <td>8.619415</td>\n",
       "      <td>-2.149524</td>\n",
       "      <td>...</td>\n",
       "      <td>5.637844</td>\n",
       "      <td>6.639583</td>\n",
       "      <td>-0.427767</td>\n",
       "      <td>6.387792</td>\n",
       "      <td>-1.583783</td>\n",
       "      <td>4.007875</td>\n",
       "      <td>-2.958319</td>\n",
       "      <td>11.536596</td>\n",
       "      <td>0.971222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91007</th>\n",
       "      <td>7.900037</td>\n",
       "      <td>-0.324087</td>\n",
       "      <td>2.305513</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.537923</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.425559</td>\n",
       "      <td>-0.324155</td>\n",
       "      <td>9.759741</td>\n",
       "      <td>-2.144938</td>\n",
       "      <td>...</td>\n",
       "      <td>6.158410</td>\n",
       "      <td>7.359038</td>\n",
       "      <td>-0.425331</td>\n",
       "      <td>7.083144</td>\n",
       "      <td>-1.580510</td>\n",
       "      <td>4.409088</td>\n",
       "      <td>-2.928202</td>\n",
       "      <td>11.536596</td>\n",
       "      <td>0.971222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90962</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>0.275824</td>\n",
       "      <td>2.241158</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.513978</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>0.102228</td>\n",
       "      <td>0.275736</td>\n",
       "      <td>8.049251</td>\n",
       "      <td>-1.840240</td>\n",
       "      <td>...</td>\n",
       "      <td>5.637844</td>\n",
       "      <td>6.279855</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>6.040116</td>\n",
       "      <td>-1.364342</td>\n",
       "      <td>4.007875</td>\n",
       "      <td>-0.985087</td>\n",
       "      <td>11.045945</td>\n",
       "      <td>0.970643</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96403</th>\n",
       "      <td>8.543516</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>1.600469</td>\n",
       "      <td>-0.153790</td>\n",
       "      <td>2.689572</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.210044</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>1.777457</td>\n",
       "      <td>-1.412953</td>\n",
       "      <td>...</td>\n",
       "      <td>7.720108</td>\n",
       "      <td>8.078493</td>\n",
       "      <td>-0.209430</td>\n",
       "      <td>7.778496</td>\n",
       "      <td>-1.291123</td>\n",
       "      <td>5.612725</td>\n",
       "      <td>-0.411472</td>\n",
       "      <td>2.707363</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90865</th>\n",
       "      <td>5.969603</td>\n",
       "      <td>0.132923</td>\n",
       "      <td>2.022741</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.411548</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>0.554220</td>\n",
       "      <td>0.132840</td>\n",
       "      <td>4.628273</td>\n",
       "      <td>-1.327460</td>\n",
       "      <td>...</td>\n",
       "      <td>4.596712</td>\n",
       "      <td>4.121489</td>\n",
       "      <td>0.397776</td>\n",
       "      <td>3.954061</td>\n",
       "      <td>-1.059111</td>\n",
       "      <td>3.205450</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>6.497628</td>\n",
       "      <td>0.968526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90999</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>-0.320464</td>\n",
       "      <td>2.285445</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.530701</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.423396</td>\n",
       "      <td>-0.320532</td>\n",
       "      <td>9.189578</td>\n",
       "      <td>-2.140723</td>\n",
       "      <td>...</td>\n",
       "      <td>5.637844</td>\n",
       "      <td>6.999310</td>\n",
       "      <td>-0.423164</td>\n",
       "      <td>6.735468</td>\n",
       "      <td>-1.577521</td>\n",
       "      <td>4.007875</td>\n",
       "      <td>-2.901409</td>\n",
       "      <td>11.536596</td>\n",
       "      <td>0.968387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95264</th>\n",
       "      <td>10.094246</td>\n",
       "      <td>2.809372</td>\n",
       "      <td>0.914017</td>\n",
       "      <td>0.384712</td>\n",
       "      <td>1.948175</td>\n",
       "      <td>1.561174</td>\n",
       "      <td>3.631831</td>\n",
       "      <td>2.809200</td>\n",
       "      <td>2.917783</td>\n",
       "      <td>-1.347619</td>\n",
       "      <td>...</td>\n",
       "      <td>9.281806</td>\n",
       "      <td>10.226143</td>\n",
       "      <td>3.639313</td>\n",
       "      <td>10.240323</td>\n",
       "      <td>-1.503470</td>\n",
       "      <td>6.816363</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>4.223469</td>\n",
       "      <td>0.966355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93907</th>\n",
       "      <td>5.326124</td>\n",
       "      <td>0.194774</td>\n",
       "      <td>0.744249</td>\n",
       "      <td>-0.114619</td>\n",
       "      <td>1.021428</td>\n",
       "      <td>1.513782</td>\n",
       "      <td>-0.115773</td>\n",
       "      <td>0.194689</td>\n",
       "      <td>2.347620</td>\n",
       "      <td>-1.222987</td>\n",
       "      <td>...</td>\n",
       "      <td>4.076147</td>\n",
       "      <td>3.761762</td>\n",
       "      <td>-0.114991</td>\n",
       "      <td>3.954061</td>\n",
       "      <td>-1.018721</td>\n",
       "      <td>2.804237</td>\n",
       "      <td>0.451857</td>\n",
       "      <td>3.465416</td>\n",
       "      <td>0.964789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90874</th>\n",
       "      <td>5.969603</td>\n",
       "      <td>-0.283773</td>\n",
       "      <td>2.098147</td>\n",
       "      <td>-0.185289</td>\n",
       "      <td>2.451266</td>\n",
       "      <td>1.892914</td>\n",
       "      <td>-0.161007</td>\n",
       "      <td>-0.283842</td>\n",
       "      <td>5.768599</td>\n",
       "      <td>-2.071793</td>\n",
       "      <td>...</td>\n",
       "      <td>4.596712</td>\n",
       "      <td>4.840945</td>\n",
       "      <td>-0.160306</td>\n",
       "      <td>4.649413</td>\n",
       "      <td>-1.533770</td>\n",
       "      <td>3.205450</td>\n",
       "      <td>-2.630041</td>\n",
       "      <td>8.013734</td>\n",
       "      <td>0.962308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93989</th>\n",
       "      <td>6.613081</td>\n",
       "      <td>-0.221542</td>\n",
       "      <td>0.936899</td>\n",
       "      <td>-0.114619</td>\n",
       "      <td>1.355057</td>\n",
       "      <td>1.627522</td>\n",
       "      <td>-0.187428</td>\n",
       "      <td>-0.221613</td>\n",
       "      <td>4.058110</td>\n",
       "      <td>-1.989687</td>\n",
       "      <td>...</td>\n",
       "      <td>5.117278</td>\n",
       "      <td>4.840945</td>\n",
       "      <td>-0.186774</td>\n",
       "      <td>4.997089</td>\n",
       "      <td>-1.481379</td>\n",
       "      <td>3.606662</td>\n",
       "      <td>-2.267499</td>\n",
       "      <td>5.739575</td>\n",
       "      <td>0.961934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93991</th>\n",
       "      <td>7.256559</td>\n",
       "      <td>-0.327062</td>\n",
       "      <td>0.993776</td>\n",
       "      <td>-0.114619</td>\n",
       "      <td>1.438464</td>\n",
       "      <td>1.651648</td>\n",
       "      <td>-0.074768</td>\n",
       "      <td>-0.327130</td>\n",
       "      <td>4.628273</td>\n",
       "      <td>-2.145851</td>\n",
       "      <td>...</td>\n",
       "      <td>5.637844</td>\n",
       "      <td>5.200672</td>\n",
       "      <td>-0.073913</td>\n",
       "      <td>5.344765</td>\n",
       "      <td>-1.581906</td>\n",
       "      <td>4.007875</td>\n",
       "      <td>-2.956748</td>\n",
       "      <td>6.497628</td>\n",
       "      <td>0.957502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cardnum_unique_count_for_card_state_1  Card_Merchdesc_State_total_7  \\\n",
       "90933                               7.256559                     -0.046803   \n",
       "94031                               7.900037                     -0.324574   \n",
       "88021                               5.326124                      0.029211   \n",
       "94135                               7.256559                     -0.082879   \n",
       "93866                               4.682646                      0.086734   \n",
       "96243                               7.900037                     -0.322862   \n",
       "90115                              -0.465180                     12.690366   \n",
       "95997                               4.039168                     -0.113518   \n",
       "96225                               7.256559                      0.130242   \n",
       "90973                               7.256559                     -0.328159   \n",
       "91007                               7.900037                     -0.324087   \n",
       "90962                               7.256559                      0.275824   \n",
       "96403                               8.543516                      0.036879   \n",
       "90865                               5.969603                      0.132923   \n",
       "90999                               7.256559                     -0.320464   \n",
       "95264                              10.094246                      2.809372   \n",
       "93907                               5.326124                      0.194774   \n",
       "90874                               5.969603                     -0.283773   \n",
       "93989                               6.613081                     -0.221542   \n",
       "93991                               7.256559                     -0.327062   \n",
       "\n",
       "       Cardnum_count_1_by_30  Cardnum_max_14  Card_dow_vdratio_0by60  \\\n",
       "90933               2.190310       -0.185289                2.493320   \n",
       "94031               1.047494       -0.114619                1.512059   \n",
       "88021               1.479820       -0.266383                2.432935   \n",
       "94135               0.557716       -0.114619                2.212960   \n",
       "93866               0.671471       -0.122573                0.869779   \n",
       "96243               2.330397       -0.188352                2.689572   \n",
       "90115               2.774478        1.493632                2.689572   \n",
       "95997               2.008820       -0.380382                2.689572   \n",
       "96225               2.298964       -0.188352                2.689572   \n",
       "90973               2.264040       -0.185289                2.522758   \n",
       "91007               2.305513       -0.185289                2.537923   \n",
       "90962               2.241158       -0.185289                2.513978   \n",
       "96403               1.600469       -0.153790                2.689572   \n",
       "90865               2.022741       -0.185289                2.411548   \n",
       "90999               2.285445       -0.185289                2.530701   \n",
       "95264               0.914017        0.384712                1.948175   \n",
       "93907               0.744249       -0.114619                1.021428   \n",
       "90874               2.098147       -0.185289                2.451266   \n",
       "93989               0.936899       -0.114619                1.355057   \n",
       "93991               0.993776       -0.114619                1.438464   \n",
       "\n",
       "       Card_dow_vdratio_0by14  Merchnum_desc_State_total_3  \\\n",
       "90933                1.892914                    -0.260007   \n",
       "94031                1.671754                    -0.425850   \n",
       "88021                1.688766                    -0.214622   \n",
       "94135                1.892914                    -0.281546   \n",
       "93866                1.450594                    -0.180278   \n",
       "96243                1.892914                    -0.424828   \n",
       "90115                1.892914                     7.344743   \n",
       "95997                1.892914                    -0.299839   \n",
       "96225                1.892914                     0.837749   \n",
       "90973                1.892914                    -0.427990   \n",
       "91007                1.892914                    -0.425559   \n",
       "90962                1.892914                     0.102228   \n",
       "96403                1.892914                    -0.210044   \n",
       "90865                1.892914                     0.554220   \n",
       "90999                1.892914                    -0.423396   \n",
       "95264                1.561174                     3.631831   \n",
       "93907                1.513782                    -0.115773   \n",
       "90874                1.892914                    -0.161007   \n",
       "93989                1.627522                    -0.187428   \n",
       "93991                1.651648                    -0.074768   \n",
       "\n",
       "       Card_Merchdesc_total_7  Card_dow_unique_count_for_merch_zip_7  \\\n",
       "90933               -0.046880                               7.479088   \n",
       "94031               -0.324642                               5.198436   \n",
       "88021                0.029132                               5.198436   \n",
       "94135               -0.082955                               2.347620   \n",
       "93866                0.086652                               1.777457   \n",
       "96243               -0.322929                               5.198436   \n",
       "90115               12.689867                              -0.503195   \n",
       "95997               -0.113593                               1.207294   \n",
       "96225                0.130159                               4.628273   \n",
       "90973               -0.328227                               8.619415   \n",
       "91007               -0.324155                               9.759741   \n",
       "90962                0.275736                               8.049251   \n",
       "96403                0.036800                               1.777457   \n",
       "90865                0.132840                               4.628273   \n",
       "90999               -0.320532                               9.189578   \n",
       "95264                2.809200                               2.917783   \n",
       "93907                0.194689                               2.347620   \n",
       "90874               -0.283842                               5.768599   \n",
       "93989               -0.221613                               4.058110   \n",
       "93991               -0.327130                               4.628273   \n",
       "\n",
       "       Cardnum_actual/toal_0  ...  Cardnum_unique_count_for_card_state_3  \\\n",
       "90933              -1.774862  ...                               5.637844   \n",
       "94031              -2.142262  ...                               6.158410   \n",
       "88021              -1.344831  ...                               4.076147   \n",
       "94135              -1.374680  ...                               7.199542   \n",
       "93866              -1.044678  ...                               3.555581   \n",
       "96243              -2.137171  ...                               6.158410   \n",
       "90115              -1.493364  ...                              -0.608947   \n",
       "95997              -1.345079  ...                               3.035015   \n",
       "96225              -1.368060  ...                               5.637844   \n",
       "90973              -2.149524  ...                               5.637844   \n",
       "91007              -2.144938  ...                               6.158410   \n",
       "90962              -1.840240  ...                               5.637844   \n",
       "96403              -1.412953  ...                               7.720108   \n",
       "90865              -1.327460  ...                               4.596712   \n",
       "90999              -2.140723  ...                               5.637844   \n",
       "95264              -1.347619  ...                               9.281806   \n",
       "93907              -1.222987  ...                               4.076147   \n",
       "90874              -2.071793  ...                               4.596712   \n",
       "93989              -1.989687  ...                               5.117278   \n",
       "93991              -2.145851  ...                               5.637844   \n",
       "\n",
       "       Cardnum_unique_count_for_card_zip_3  Merchnum_desc_Zip_total_3  \\\n",
       "90933                             5.920128                  -0.259483   \n",
       "94031                             5.560400                  -0.425622   \n",
       "88021                             3.761762                  -0.214017   \n",
       "94135                             7.359038                  -0.281060   \n",
       "93866                             3.402034                  -0.179612   \n",
       "96243                             5.560400                  -0.424598   \n",
       "90115                            -0.554969                   7.358863   \n",
       "95997                             3.042307                  -0.299386   \n",
       "96225                             5.200672                   0.840236   \n",
       "90973                             6.639583                  -0.427767   \n",
       "91007                             7.359038                  -0.425331   \n",
       "90962                             6.279855                   0.103400   \n",
       "96403                             8.078493                  -0.209430   \n",
       "90865                             4.121489                   0.397776   \n",
       "90999                             6.999310                  -0.423164   \n",
       "95264                            10.226143                   3.639313   \n",
       "93907                             3.761762                  -0.114991   \n",
       "90874                             4.840945                  -0.160306   \n",
       "93989                             4.840945                  -0.186774   \n",
       "93991                             5.200672                  -0.073913   \n",
       "\n",
       "       Cardnum_unique_count_for_Merchnum_3  Cardnum_actual/toal_1  \\\n",
       "90933                             5.692441              -1.325927   \n",
       "94031                             5.692441              -1.579579   \n",
       "88021                             3.606385              -0.886161   \n",
       "94135                             7.778496              -1.364339   \n",
       "93866                             3.606385              -1.002588   \n",
       "96243                             5.344765              -1.575458   \n",
       "90115                            -0.565726              -1.449263   \n",
       "95997                             2.911033              -1.151123   \n",
       "96225                             4.997089              -1.047553   \n",
       "90973                             6.387792              -1.583783   \n",
       "91007                             7.083144              -1.580510   \n",
       "90962                             6.040116              -1.364342   \n",
       "96403                             7.778496              -1.291123   \n",
       "90865                             3.954061              -1.059111   \n",
       "90999                             6.735468              -1.577521   \n",
       "95264                            10.240323              -1.503470   \n",
       "93907                             3.954061              -1.018721   \n",
       "90874                             4.649413              -1.533770   \n",
       "93989                             4.997089              -1.481379   \n",
       "93991                             5.344765              -1.581906   \n",
       "\n",
       "       Cardnum_unique_count_for_card_state_7  Cardnum_actual/max_0  \\\n",
       "90933                               4.007875             -0.877403   \n",
       "94031                               4.409088             -2.940496   \n",
       "88021                               3.205450              0.166403   \n",
       "94135                               5.211513             -0.433736   \n",
       "93866                               2.403025             -0.217180   \n",
       "96243                               4.409088             -2.918637   \n",
       "90115                              -0.806676             -0.303934   \n",
       "95997                               2.001812             -0.248524   \n",
       "96225                               4.007875              0.451857   \n",
       "90973                               4.007875             -2.958319   \n",
       "91007                               4.409088             -2.928202   \n",
       "90962                               4.007875             -0.985087   \n",
       "96403                               5.612725             -0.411472   \n",
       "90865                               3.205450              0.451857   \n",
       "90999                               4.007875             -2.901409   \n",
       "95264                               6.816363              0.451857   \n",
       "93907                               2.804237              0.451857   \n",
       "90874                               3.205450             -2.630041   \n",
       "93989                               3.606662             -2.267499   \n",
       "93991                               4.007875             -2.956748   \n",
       "\n",
       "       Card_dow_unique_count_for_merch_state_1  predicted  Fraud  \n",
       "90933                                10.287893   0.990481      1  \n",
       "94031                                 7.255681   0.980239      1  \n",
       "88021                                 7.255681   0.978587      1  \n",
       "94135                                 3.465416   0.977696      1  \n",
       "93866                                 2.707363   0.976888      1  \n",
       "96243                                 7.255681   0.973769      1  \n",
       "90115                                -0.324849   0.972978      1  \n",
       "95997                                 1.949310   0.972401      0  \n",
       "96225                                 6.497628   0.971630      1  \n",
       "90973                                11.536596   0.971222      1  \n",
       "91007                                11.536596   0.971222      1  \n",
       "90962                                11.045945   0.970643      1  \n",
       "96403                                 2.707363   0.970516      1  \n",
       "90865                                 6.497628   0.968526      1  \n",
       "90999                                11.536596   0.968387      1  \n",
       "95264                                 4.223469   0.966355      0  \n",
       "93907                                 3.465416   0.964789      1  \n",
       "90874                                 8.013734   0.962308      1  \n",
       "93989                                 5.739575   0.961934      1  \n",
       "93991                                 6.497628   0.957502      1  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['bin','#recs','#g','#b','%g','%b','tot','cg','cb','%cg','FDR','KS','FPR']\n",
    "FDR_trn = pd.DataFrame(np.zeros((101, 13)), columns = cols)\n",
    "FDR_tst = pd.DataFrame(np.zeros((101, 13)), columns = cols)\n",
    "FDR_oot = pd.DataFrame(np.zeros((101, 13)), columns = cols)\n",
    "trn_sorted = X_trn_eval.sort_values('predicted',ascending=False)\n",
    "tst_sorted = X_tst_eval.sort_values('predicted',ascending=False)\n",
    "oot_sorted = X_oot_eval.sort_values('predicted',ascending=False)\n",
    "bad_tot_trn = sum(X_trn_eval.loc[:, 'Fraud'])\n",
    "bad_tot_tst = sum(X_tst_eval.loc[:, 'Fraud'])\n",
    "bad_tot_oot = sum(X_oot_eval.loc[:, 'Fraud'])\n",
    "num_tot_trn = len(X_trn_eval)\n",
    "num_tot_tst = len(X_tst_eval)\n",
    "num_tot_oot = len(X_oot_eval)\n",
    "good_tot_trn = num_tot_trn - bad_tot_trn\n",
    "good_tot_tst = num_tot_tst - bad_tot_tst\n",
    "good_tot_oot = num_tot_oot - bad_tot_oot\n",
    "oot_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>#recs</th>\n",
       "      <th>#g</th>\n",
       "      <th>#b</th>\n",
       "      <th>%g</th>\n",
       "      <th>%b</th>\n",
       "      <th>tot</th>\n",
       "      <th>cg</th>\n",
       "      <th>cb</th>\n",
       "      <th>%cg</th>\n",
       "      <th>FDR</th>\n",
       "      <th>KS</th>\n",
       "      <th>FPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.032787</td>\n",
       "      <td>81.967213</td>\n",
       "      <td>122.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>33.670034</td>\n",
       "      <td>33.485702</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>60.975610</td>\n",
       "      <td>39.024390</td>\n",
       "      <td>245.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.812736</td>\n",
       "      <td>49.831650</td>\n",
       "      <td>49.018914</td>\n",
       "      <td>0.655405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>78.688525</td>\n",
       "      <td>21.311475</td>\n",
       "      <td>367.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1.617093</td>\n",
       "      <td>58.585859</td>\n",
       "      <td>56.968766</td>\n",
       "      <td>1.109195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>90.163934</td>\n",
       "      <td>9.836066</td>\n",
       "      <td>489.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>2.538752</td>\n",
       "      <td>62.626263</td>\n",
       "      <td>60.087511</td>\n",
       "      <td>1.629032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11743.0</td>\n",
       "      <td>11446.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>95.902807</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.097193</td>\n",
       "      <td>38.538721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11865.0</td>\n",
       "      <td>11568.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>96.925010</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.074990</td>\n",
       "      <td>38.949495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11987.0</td>\n",
       "      <td>11690.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>97.947214</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.052786</td>\n",
       "      <td>39.360269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12110.0</td>\n",
       "      <td>11813.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>98.977796</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.022204</td>\n",
       "      <td>39.774411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12232.0</td>\n",
       "      <td>11935.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.185185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bin  #recs     #g     #b          %g         %b      tot       cg  \\\n",
       "0      0.0    0.0    0.0    0.0    0.000000   0.000000      0.0      0.0   \n",
       "1      1.0  122.0   22.0  100.0   18.032787  81.967213    122.0     22.0   \n",
       "2      2.0  123.0   75.0   48.0   60.975610  39.024390    245.0     97.0   \n",
       "3      3.0  122.0   96.0   26.0   78.688525  21.311475    367.0    193.0   \n",
       "4      4.0  122.0  110.0   12.0   90.163934   9.836066    489.0    303.0   \n",
       "..     ...    ...    ...    ...         ...        ...      ...      ...   \n",
       "96    96.0  123.0  123.0    0.0  100.000000   0.000000  11743.0  11446.0   \n",
       "97    97.0  122.0  122.0    0.0  100.000000   0.000000  11865.0  11568.0   \n",
       "98    98.0  122.0  122.0    0.0  100.000000   0.000000  11987.0  11690.0   \n",
       "99    99.0  123.0  123.0    0.0  100.000000   0.000000  12110.0  11813.0   \n",
       "100  100.0  122.0  122.0    0.0  100.000000   0.000000  12232.0  11935.0   \n",
       "\n",
       "        cb         %cg         FDR         KS        FPR  \n",
       "0      0.0    0.000000    0.000000   0.000000   0.000000  \n",
       "1    100.0    0.184332   33.670034  33.485702   0.220000  \n",
       "2    148.0    0.812736   49.831650  49.018914   0.655405  \n",
       "3    174.0    1.617093   58.585859  56.968766   1.109195  \n",
       "4    186.0    2.538752   62.626263  60.087511   1.629032  \n",
       "..     ...         ...         ...        ...        ...  \n",
       "96   297.0   95.902807  100.000000   4.097193  38.538721  \n",
       "97   297.0   96.925010  100.000000   3.074990  38.949495  \n",
       "98   297.0   97.947214  100.000000   2.052786  39.360269  \n",
       "99   297.0   98.977796  100.000000   1.022204  39.774411  \n",
       "100  297.0  100.000000  100.000000   0.000000  40.185185  \n",
       "\n",
       "[101 rows x 13 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_fraud = 1000\n",
    "cost_fp = 30\n",
    "for i in range(101):\n",
    "    percent_rows_trn = int(round(X_trn_eval.shape[0]*0.01*i))\n",
    "    percent_rows_tst = int(round(X_tst_eval.shape[0]*0.01*i))\n",
    "    percent_rows_oot = int(round(X_oot_eval.shape[0]*0.01*i))\n",
    "    temp_trn = trn_sorted.head(percent_rows_trn)\n",
    "    temp_tst = tst_sorted.head(percent_rows_tst)\n",
    "    temp_oot = oot_sorted.head(percent_rows_oot)\n",
    "    num_bad_trn = sum(temp_trn.loc[:,'Fraud'])\n",
    "    num_bad_tst = sum(temp_tst.loc[:,'Fraud'])\n",
    "    num_bad_oot = sum(temp_oot.loc[:,'Fraud'])\n",
    "    num_tot_trn = len(temp_trn)\n",
    "    num_tot_tst = len(temp_tst)\n",
    "    num_tot_oot = len(temp_oot)\n",
    "    num_good_trn = num_tot_trn - num_bad_trn\n",
    "    num_good_tst = num_tot_tst - num_bad_tst\n",
    "    num_good_oot = num_tot_oot - num_bad_oot\n",
    "    \n",
    "    FDR_trn.loc[i, 'bin'] = i\n",
    "    FDR_trn.loc[i,'#recs'] = 0\n",
    "    FDR_trn.loc[i, 'tot'] = num_tot_trn\n",
    "    FDR_trn.loc[i, 'cg'] = num_good_trn\n",
    "    FDR_trn.loc[i, 'cb'] = num_bad_trn\n",
    "    FDR_trn.loc[i, 'Fraud Savings'] = FDR_trn.loc[i, 'cb'] * cost_fraud\n",
    "    FDR_trn.loc[i, 'FP Loss'] = FDR_trn.loc[i, 'cg'] * cost_fp\n",
    "    FDR_trn.loc[i, 'Overall Savings'] = FDR_trn.loc[i, 'Fraud Savings'] - FDR_trn.loc[i, 'FP Loss']\n",
    "    FDR_tst.loc[i, 'bin'] = i\n",
    "    FDR_tst.loc[i, 'tot'] = num_tot_tst\n",
    "    FDR_tst.loc[i, 'cg'] = num_good_tst\n",
    "    FDR_tst.loc[i, 'cb'] = num_bad_tst\n",
    "    FDR_tst.loc[i, 'Fraud Savings'] = FDR_tst.loc[i, 'cb'] * cost_fraud\n",
    "    FDR_tst.loc[i, 'FP Loss'] = FDR_tst.loc[i, 'cg'] * cost_fp\n",
    "    FDR_tst.loc[i, 'Overall Savings'] = FDR_tst.loc[i, 'Fraud Savings'] - FDR_tst.loc[i, 'FP Loss']\n",
    "    FDR_oot.loc[i, 'bin'] = i\n",
    "    FDR_oot.loc[i, 'tot'] = num_tot_oot\n",
    "    FDR_oot.loc[i, 'cg'] = num_good_oot\n",
    "    FDR_oot.loc[i, 'cb'] = num_bad_oot\n",
    "\n",
    "    if i != 0:\n",
    "        FDR_trn.loc[i, '#g'] = num_good_trn - FDR_trn.loc[i-1, 'cg']\n",
    "        FDR_trn.loc[i, '#b'] = num_bad_trn - FDR_trn.loc[i-1, 'cb']\n",
    "        FDR_trn.loc[i,'#recs'] = FDR_trn.loc[i, '#g'] + FDR_trn.loc[i, '#b']\n",
    "        FDR_trn.loc[i, '%g'] = 100* (num_good_trn - FDR_trn.loc[i-1, 'cg']) / (num_tot_trn - FDR_trn.loc[i-1, 'tot'])\n",
    "        FDR_trn.loc[i, '%b'] = 100 - FDR_trn.loc[i, '%g']\n",
    "        FDR_trn.loc[i, '%cg'] = 100 * num_good_trn / good_tot_trn\n",
    "        FDR_trn.loc[i, 'FDR'] = 100 * num_bad_trn / bad_tot_trn\n",
    "        FDR_trn.loc[i, 'KS'] = FDR_trn.loc[i, 'FDR'] - FDR_trn.loc[i, '%cg']\n",
    "        FDR_trn.loc[i, 'FPR'] = num_good_trn / num_bad_trn\n",
    "        FDR_tst.loc[i, '#g'] = num_good_tst - FDR_tst.loc[i-1, 'cg']\n",
    "        FDR_tst.loc[i, '#b'] = num_bad_tst - FDR_tst.loc[i-1, 'cb']\n",
    "        FDR_tst.loc[i,'#recs'] = FDR_tst.loc[i, '#g'] + FDR_tst.loc[i, '#b']\n",
    "        FDR_tst.loc[i, '%g'] = 100* (num_good_tst - FDR_tst.loc[i-1, 'cg']) / (num_tot_tst - FDR_tst.loc[i-1, 'tot'])\n",
    "        FDR_tst.loc[i, '%b'] = 100 - FDR_tst.loc[i, '%g']\n",
    "        FDR_tst.loc[i, '%cg'] = 100 * num_good_tst / good_tot_tst\n",
    "        FDR_tst.loc[i, 'FDR'] = 100 * num_bad_tst / bad_tot_tst\n",
    "        FDR_tst.loc[i, 'KS'] = FDR_tst.loc[i, 'FDR'] - FDR_tst.loc[i, '%cg']\n",
    "        FDR_tst.loc[i, 'FPR'] = num_good_tst / num_bad_tst\n",
    "        FDR_oot.loc[i, '#g'] = num_good_oot - FDR_oot.loc[i-1, 'cg']\n",
    "        FDR_oot.loc[i, '#b'] = num_bad_oot - FDR_oot.loc[i-1, 'cb']\n",
    "        FDR_oot.loc[i,'#recs'] = FDR_oot.loc[i, '#g'] + FDR_oot.loc[i, '#b']\n",
    "        FDR_oot.loc[i, '%g'] = 100* (num_good_oot - FDR_oot.loc[i-1, 'cg']) / (num_tot_oot - FDR_oot.loc[i-1, 'tot'])\n",
    "        FDR_oot.loc[i, '%b'] = 100 - FDR_oot.loc[i, '%g']\n",
    "        FDR_oot.loc[i, '%cg'] = 100 * num_good_oot / good_tot_oot\n",
    "        FDR_oot.loc[i, 'FDR'] = 100 * num_bad_oot / bad_tot_oot\n",
    "        FDR_oot.loc[i, 'KS'] = FDR_oot.loc[i, 'FDR'] - FDR_oot.loc[i, '%cg']\n",
    "        FDR_oot.loc[i, 'FPR'] = num_good_oot / num_bad_oot\n",
    "\n",
    "FDR_oot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max possible savings: 47,604,000.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_fraud = 400\n",
    "cost_fp = 20\n",
    "# xmult: oot is only 2 out of 12 months. 100,000 sample transactions out of 10 million/year\n",
    "xoot = 12/2 * 10000000/100000\n",
    "Financials_trn = pd.DataFrame(np.zeros((101, 3)), columns = ['Fraud Savings','FP Loss','Overall Savings'])\n",
    "Financials_tst = pd.DataFrame(np.zeros((101, 3)), columns = ['Fraud Savings','FP Loss','Overall Savings'])\n",
    "Financials_oot = pd.DataFrame(np.zeros((101, 3)), columns = ['Fraud Savings','FP Loss','Overall Savings'])\n",
    "for i in range(101):\n",
    "    Financials_trn.loc[i, 'Fraud Savings'] = FDR_trn.loc[i, 'cb'] * cost_fraud * xoot\n",
    "    Financials_trn.loc[i, 'FP Loss'] = FDR_trn.loc[i, 'cg'] * cost_fp * xoot\n",
    "    Financials_trn.loc[i, 'Overall Savings'] = Financials_trn.loc[i, 'Fraud Savings'] - Financials_trn.loc[i, 'FP Loss']\n",
    "    Financials_tst.loc[i, 'Fraud Savings'] = FDR_tst.loc[i, 'cb'] * cost_fraud * xoot\n",
    "    Financials_tst.loc[i, 'FP Loss'] = FDR_tst.loc[i, 'cg'] * cost_fp * xoot\n",
    "    Financials_tst.loc[i, 'Overall Savings'] = Financials_tst.loc[i, 'Fraud Savings'] - Financials_tst.loc[i, 'FP Loss']\n",
    "    Financials_oot.loc[i, 'Fraud Savings'] = FDR_oot.loc[i, 'cb'] * cost_fraud * xoot\n",
    "    Financials_oot.loc[i, 'FP Loss'] = FDR_oot.loc[i, 'cg'] * cost_fp * xoot\n",
    "    Financials_oot.loc[i, 'Overall Savings'] = Financials_oot.loc[i, 'Fraud Savings'] - Financials_oot.loc[i, 'FP Loss']\n",
    "\n",
    "max_savings = Financials_oot['Overall Savings'].max(0)\n",
    "print('Max possible savings: '+'{:,}'.format(max_savings))\n",
    "yupper = max_savings * 1.5\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(Financials_oot['Fraud Savings'], color='green')\n",
    "plt.plot(Financials_oot['FP Loss'], color='red')\n",
    "plt.plot(Financials_oot['Overall Savings'], color='blue')\n",
    "xlimit = 20\n",
    "interval = 1\n",
    "plt.xlim(0,xlimit)\n",
    "plt.ylim(0,yupper)\n",
    "plt.xticks(ticks=np.linspace(0,xlimit, num=int(xlimit/interval)+1))\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.savefig('savings.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>#recs</th>\n",
       "      <th>#g</th>\n",
       "      <th>#b</th>\n",
       "      <th>%g</th>\n",
       "      <th>%b</th>\n",
       "      <th>tot</th>\n",
       "      <th>cg</th>\n",
       "      <th>cb</th>\n",
       "      <th>%cg</th>\n",
       "      <th>FDR</th>\n",
       "      <th>KS</th>\n",
       "      <th>FPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.032787</td>\n",
       "      <td>81.967213</td>\n",
       "      <td>122.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>33.670034</td>\n",
       "      <td>33.485702</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>60.975610</td>\n",
       "      <td>39.024390</td>\n",
       "      <td>245.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.812736</td>\n",
       "      <td>49.831650</td>\n",
       "      <td>49.018914</td>\n",
       "      <td>0.655405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>78.688525</td>\n",
       "      <td>21.311475</td>\n",
       "      <td>367.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1.617093</td>\n",
       "      <td>58.585859</td>\n",
       "      <td>56.968766</td>\n",
       "      <td>1.109195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>90.163934</td>\n",
       "      <td>9.836066</td>\n",
       "      <td>489.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>2.538752</td>\n",
       "      <td>62.626263</td>\n",
       "      <td>60.087511</td>\n",
       "      <td>1.629032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>88.617886</td>\n",
       "      <td>11.382114</td>\n",
       "      <td>612.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3.452032</td>\n",
       "      <td>67.340067</td>\n",
       "      <td>63.888036</td>\n",
       "      <td>2.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>85.245902</td>\n",
       "      <td>14.754098</td>\n",
       "      <td>734.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>4.323419</td>\n",
       "      <td>73.400673</td>\n",
       "      <td>69.077255</td>\n",
       "      <td>2.366972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.901639</td>\n",
       "      <td>4.098361</td>\n",
       "      <td>856.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>5.303729</td>\n",
       "      <td>75.084175</td>\n",
       "      <td>69.780447</td>\n",
       "      <td>2.838565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>93.495935</td>\n",
       "      <td>6.504065</td>\n",
       "      <td>979.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>6.267281</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>71.510497</td>\n",
       "      <td>3.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.901639</td>\n",
       "      <td>4.098361</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>7.247591</td>\n",
       "      <td>79.461279</td>\n",
       "      <td>72.213688</td>\n",
       "      <td>3.665254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin  #recs     #g     #b         %g         %b     tot     cg     cb  \\\n",
       "0  0.0    0.0    0.0    0.0   0.000000   0.000000     0.0    0.0    0.0   \n",
       "1  1.0  122.0   22.0  100.0  18.032787  81.967213   122.0   22.0  100.0   \n",
       "2  2.0  123.0   75.0   48.0  60.975610  39.024390   245.0   97.0  148.0   \n",
       "3  3.0  122.0   96.0   26.0  78.688525  21.311475   367.0  193.0  174.0   \n",
       "4  4.0  122.0  110.0   12.0  90.163934   9.836066   489.0  303.0  186.0   \n",
       "5  5.0  123.0  109.0   14.0  88.617886  11.382114   612.0  412.0  200.0   \n",
       "6  6.0  122.0  104.0   18.0  85.245902  14.754098   734.0  516.0  218.0   \n",
       "7  7.0  122.0  117.0    5.0  95.901639   4.098361   856.0  633.0  223.0   \n",
       "8  8.0  123.0  115.0    8.0  93.495935   6.504065   979.0  748.0  231.0   \n",
       "9  9.0  122.0  117.0    5.0  95.901639   4.098361  1101.0  865.0  236.0   \n",
       "\n",
       "        %cg        FDR         KS       FPR  \n",
       "0  0.000000   0.000000   0.000000  0.000000  \n",
       "1  0.184332  33.670034  33.485702  0.220000  \n",
       "2  0.812736  49.831650  49.018914  0.655405  \n",
       "3  1.617093  58.585859  56.968766  1.109195  \n",
       "4  2.538752  62.626263  60.087511  1.629032  \n",
       "5  3.452032  67.340067  63.888036  2.060000  \n",
       "6  4.323419  73.400673  69.077255  2.366972  \n",
       "7  5.303729  75.084175  69.780447  2.838565  \n",
       "8  6.267281  77.777778  71.510497  3.238095  \n",
       "9  7.247591  79.461279  72.213688  3.665254  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FDR_oot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR.to_csv('FDR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDR_trn.to_csv('FDR_trn.csv', index=False)\n",
    "FDR_tst.to_csv('FDR_tst.csv', index=False)\n",
    "FDR_oot.to_csv('FDR_oot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58463.0 1221.0 59684.0 59684.0\n"
     ]
    }
   ],
   "source": [
    "Metrics = FDR_trn.copy()\n",
    "num_good = Metrics['#g'].sum()\n",
    "num_bad = Metrics['#b'].sum()\n",
    "num_tot = Metrics['#recs'].sum()\n",
    "print(num_good,num_bad,num_good+num_bad,num_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics['TP'] = Metrics['cb']\n",
    "Metrics['TN'] = num_good - Metrics['cg']\n",
    "Metrics['FN'] = num_bad - Metrics['cb']\n",
    "Metrics['FP'] = Metrics['cg']\n",
    "Metrics['Accuracy'] = (Metrics['TP'] + Metrics['TN']) / num_tot\n",
    "Metrics['Misclass'] = (Metrics['FP'] + Metrics['FN']) / num_tot\n",
    "Metrics['FPRate'] = Metrics['FP'] / (Metrics['FP'] + Metrics['TN'])\n",
    "Metrics['FPRatio'] = Metrics['FP'] / Metrics['TP']\n",
    "Metrics['TPR'] = Metrics['TP'] / (Metrics['TP'] + Metrics['FN'])\n",
    "Metrics['TNR'] = Metrics['TN'] / (Metrics['TN'] + Metrics['FP'])\n",
    "Metrics['Precision'] = Metrics['TP'] / (Metrics['TP'] + Metrics['FP'])\n",
    "Metrics['f1'] = 2 * Metrics['Precision'] * Metrics['TPR'] / (Metrics['Precision'] + Metrics['TPR'])\n",
    "Metrics.to_csv('Metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6739/4042720319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'font.size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TPR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fraud Detection Rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1360\u001b[0m                     message='Support for multi-dimensional indexing')\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m                 \u001b[0;31m# we have definitely hit a pandas index or series object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;31m# cast to a numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rows_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1161\u001b[0m             )\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;31m# the asarray is needed to avoid returning a 2D DatetimeArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mdisallow_ndim_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexers/utils.py\u001b[0m in \u001b[0;36mdisallow_ndim_indexing\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \"\"\"\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;34m\"Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;34m\"supported. Convert to a numpy array before indexing instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size':10})\n",
    "plt.plot(Metrics['bin'], Metrics['TPR'])\n",
    "plt.title('Fraud Detection Rate', fontsize=20)\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Score cutoff %', fontsize=15)\n",
    "plt.ylabel('FDR', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a highly imbalanced problem the # goods in each bin is close to constant except for the first few bins, so the FDR curve (xaxis is bin #) and the ROC (x axis is FP, which is # goods below the cutoff) look very similar. They're only different in the first few bins where the # goods are not ~constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Metrics['FPRate'], Metrics['TPR'])\n",
    "plt.title('ROC', fontsize=20)\n",
    "plt.xlabel('FPRate',fontsize=15)\n",
    "plt.ylabel('TPRate', fontsize=15)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"duration: \", datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('card transactions.csv')\n",
    "df.dropna(how='all', axis=1, inplace=True)\n",
    "df = df[df['Transtype'] == 'P']\n",
    "df = df[df['Amount'] <= 3000000]\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prediction'] = model.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scale = 1/df['prediction'].max()\n",
    "df['prediction'] = df['prediction']*pred_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=['prediction'],ascending=False)\n",
    "df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = df[df['Fraud']==1]\n",
    "bads['Cardnum'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bads['Merchnum'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = 5142140316\n",
    "# card = 5142847398\n",
    "# card = 5142199009\n",
    "# card = 5142160778\n",
    "# card = 5142189341\n",
    "# card = 5142181728\n",
    "# card = 5142212038\n",
    "# card = 5142220919\n",
    "# card = 5142214614\n",
    "# card = 5142202847\n",
    "# card = 5142138135\n",
    "# card = 5142271065\n",
    "# card = 5142152857\n",
    "# card = 5142179617\n",
    "# card = 5142235211\n",
    "# card = 5142197711\n",
    "# card = 5142182128\n",
    "# card = 5142189113\n",
    "# card = 5142197563 \n",
    "\n",
    "merch = 4353000719908\n",
    "# merch = 930090121224  \n",
    "# merch = 48834000695423\n",
    "# merch = 44503738417400\n",
    "# merch = 44620009957157\n",
    "# merch = 4618901687330\n",
    "# merch = 4900009045549\n",
    "# merch = 49108234610000\n",
    "# merch = 4253052983001\n",
    "# merch = 4938909877224\n",
    "# merch = 44503082476300\n",
    "# merch = 46006333528866\n",
    "# merch = 4997674930332\n",
    "# merch = 46070095870009\n",
    "# merch = 49900020006406\n",
    "# sample = df[df['Cardnum'] == card]\n",
    "sample = df[df['Merchnum'] == str(merch)]\n",
    "sample['Fraud'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.plot(sample['Date'],sample['prediction'])\n",
    "plt.plot_date(sample['Date'],sample['Fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsample = sample[sample['Date'] > '2010-11-09']\n",
    "tsample = tsample[tsample['Date'] < '2010-12-01']\n",
    "tsample.reset_index(inplace=True)\n",
    "tsample.reset_index(inplace=True)\n",
    "tsample.rename(columns={'level_0':'counter'},inplace=True)\n",
    "tsample['counter'] = tsample['counter']+1\n",
    "tsample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.plot(tsample['Date'],tsample['prediction'])\n",
    "plt.plot_date(tsample['Date'],tsample['Fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.plot(tsample['counter'],tsample['prediction'])\n",
    "plt.scatter(tsample['counter'],tsample['Fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsample.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 700\n",
    "delta = 40\n",
    "odds_at_base = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_trn_eval,X_tst_eval,X_oot_eval])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib = df.iloc[:,df.shape[1]-2:]\n",
    "trn_sorted = X_trn_eval.sort_values('predicted',ascending=True)\n",
    "calib = calib.sort_values('predicted')\n",
    "calib.rename(columns={'predicted':'score_raw'}, inplace=True)\n",
    "calib['score_raw'].clip(upper=.999, inplace=True)\n",
    "calib['score_raw'].clip(lower=.00001, inplace=True)\n",
    "calib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(calib['score_raw'],bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason we bin the records is to calculate the odds. We can then find the relationship between the raw score and the log odds.\n",
    "nbins=100\n",
    "cols = ['bin','#recs','#g','#b','%g','%b','tot','cg','cb','score_raw','prob(%)','odds','log_odds', 'log_odds_adj']\n",
    "cal_bins = pd.DataFrame(np.zeros((nbins+1, 14)), columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tot = sum(calib.loc[:, 'Fraud'])\n",
    "good_tot = len(calib) - bad_tot\n",
    "print(bad_tot, good_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1/nbins\n",
    "nrecs = calib.shape[0]\n",
    "cal_bins.loc[0,'log_odds_adj'] = -8\n",
    "for i in range(nbins+1):\n",
    "    percent_rows_top = int(round(nrecs*frac*i))\n",
    "    percent_rows_bottom = max(int(round(nrecs*frac*(i-1))),0)\n",
    "    temp = calib.iloc[percent_rows_bottom:percent_rows_top,:]\n",
    "    cal_bins.loc[i, 'score_raw'] = temp['score_raw'].mean()\n",
    "    num_bad = int(sum(temp.loc[:,'Fraud']))\n",
    "    num_tot = len(temp) * i\n",
    "    num_good = int(num_tot - num_bad)\n",
    "    cal_bins.loc[i, 'bin'] = i\n",
    "    cal_bins.loc[i,'#recs'] = 0\n",
    "    cal_bins.loc[i, 'tot'] = num_tot\n",
    "    cal_bins.loc[i, 'cg'] = num_good\n",
    "    cal_bins.loc[i, 'cb'] = num_bad\n",
    "    if i != 0:\n",
    "        cal_bins.loc[i, '#recs'] = len(temp)\n",
    "        cal_bins.loc[i, '#b'] = int(sum(temp.loc[:, 'Fraud']))\n",
    "        cal_bins.loc[i, '#g'] = cal_bins.loc[i, '#recs'] - cal_bins.loc[i, '#b']\n",
    "        cal_bins.loc[i, '%g'] = 100* cal_bins.loc[i, '#g'] / cal_bins.loc[i, '#recs']\n",
    "        cal_bins.loc[i, '%b'] = 100 - cal_bins.loc[i, '%g']\n",
    "        cal_bins.loc[i, 'cg'] = cal_bins.loc[i-1, 'cg'] + cal_bins.loc[i, '#g']\n",
    "        cal_bins.loc[i, 'cb'] = cal_bins.loc[i-1, 'cb'] + cal_bins.loc[i, '#b']\n",
    "        cal_bins.loc[i, 'prob(%)'] = 100 * cal_bins.loc[i, '#b'] / cal_bins.loc[i, '#recs']\n",
    "        cal_bins.loc[i, 'odds'] = (cal_bins.loc[i, '#b'] + .001) / cal_bins.loc[i, '#g']\n",
    "        cal_bins.loc[i, 'log_odds'] = np.log(cal_bins.loc[i, 'odds'])\n",
    "        cal_bins.loc[i, 'log_odds_adj'] = max(cal_bins.loc[i, 'log_odds'], cal_bins.loc[i-1, 'log_odds_adj'])\n",
    "        \n",
    "cal_bins.drop(index=0,axis=0,inplace=True)\n",
    "cal_bins.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_bins.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cal_bins['score_raw'],cal_bins['log_odds'])\n",
    "plt.plot(cal_bins['score_raw'],cal_bins['log_odds_adj'])\n",
    "# plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "    return c + b/(1 + np.exp(-a * x))\n",
    "\n",
    "def funcinv(x, a, b, c):\n",
    "    return -np.log(-1 + b/(x - c))/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "popt, pcov = curve_fit(func, cal_bins['log_odds_adj'],cal_bins['score_raw'])   \n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(cal_bins['log_odds'],cal_bins['score'])\n",
    "plt.plot(cal_bins['log_odds_adj'],cal_bins['score_raw'])\n",
    "plt.plot(cal_bins['log_odds_adj'],func(cal_bins['log_odds_adj'], *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the smooth curve fit to see if it's OK.\n",
    "plt.plot(cal_bins['score_raw'], cal_bins['log_odds_adj'])\n",
    "plt.plot(cal_bins['score_raw'], funcinv(cal_bins['score_raw'], *popt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fit looks pretty good\n",
    "cal_bins['fit'] = funcinv(cal_bins['score_raw'], *popt)\n",
    "cal_bins.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the calibrated score\n",
    "cal_bins['score_calib'] = base + delta * cal_bins['fit'] - delta * np.log2(odds_at_base)\n",
    "cal_bins['score_calib'].fillna(999, inplace=True)\n",
    "cal_bins['score_calib'].clip(upper=999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib['score_calib'] = base - delta * np.log2(odds_at_base) + delta * funcinv(calib['score_raw'], *popt)\n",
    "calib['score_calib'].fillna(999, inplace=True)\n",
    "calib['score_calib'].clip(upper=999, inplace=True)\n",
    "calib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nbins=100\n",
    "plt.hist(calib['score_raw'],bins=nbins)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(calib['score_calib'],bins=nbins)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods = calib[calib['Fraud']==0]\n",
    "bads = calib[calib['Fraud']==1]\n",
    "plt.hist(goods['score_raw'],bins = nbins, alpha = .5)\n",
    "plt.hist(bads['score_raw'],bins = nbins, alpha = .5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(goods['score_calib'],bins = nbins, alpha = .5)\n",
    "plt.hist(bads['score_calib'],bins = nbins, alpha = .5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"duration: \", datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
